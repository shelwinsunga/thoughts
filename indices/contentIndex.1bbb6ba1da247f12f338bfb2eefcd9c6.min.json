{"/":{"title":"Notes","content":"\nA catalog of notes, thoughts, and other digital type that I have created or consumed.\n\n\n### [† MIT 6.824: Distributed Systems](tags/6.824)\n### [CS61B: Data Structures and Algorithms](tags/Data-Structures)\n\n\n\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/comp/cowsignal":{"title":"Cow Signal","content":"\nUSACO 2016 December Contest, Bronze - Problem 3. The Cow-Signal\n# Problem Statement\n\nBessie and her cow friends are playing as their favorite cow superheroes. Of course, everyone knows that any self-respecting superhero needs a signal to call them to action. Bessie has drawn a special signal on a sheet of $M×N$ paper $(1≤M≤10,1≤N≤10)$, but this is too small, much too small! Bessie wants to amplify the signal so it is exactly $K$ times bigger $(1≤K≤10)$ in each direction.\n\n#### Input Format (cowsignal.in)\n\nThe first line of input contains $M$,$N$, and $K$, separated by spaces.\nThe next $M$ lines each contain a length-$N$ string, collectively describing the picture of the signal.\n\n#### Output Format (cowsignal.out)\n\nYou should output $KM$ lines, each with $KN$ characters, giving a picture of the enlarged signal.\n\n\n#### Sample Input: \n5 4 2   \u003cbr\u003e\nXXX.    \u003cbr\u003e\nX..X    \u003cbr\u003e\nXXX.    \u003cbr\u003e\nX..X    \u003cbr\u003e\nXXX.    \u003cbr\u003e\n#### Sample Output:\n\nXXXXXX..    \u003cbr\u003e\nXXXXXX..    \u003cbr\u003e\nXX....XX    \u003cbr\u003e\nXX....XX    \u003cbr\u003e\nXXXXXX..    \u003cbr\u003e\nXXXXXX..    \u003cbr\u003e\nXX....XX    \u003cbr\u003e\nXX....XX    \u003cbr\u003e\nXXXXXX..    \u003cbr\u003e\nXXXXXX..    \u003cbr\u003e\n\n# My Solution\n\u003cbr\u003e\n\n```\n#include \u003ciostream\u003e\n#include \u003cvector\u003e\n#include \u003cstring\u003e\nusing namespace std;\n\nint main()\n{\n    freopen(\"cowsignal.in\", \"r\", stdin);\n\tfreopen(\"cowsignal.out\", \"w\", stdout);\n    int m,n,k;\n    scanf(\"%d %d %d\", \u0026m, \u0026n, \u0026k);\n    string s; \n    vector\u003cstring\u003e signal;\n    for(int i = 0; i \u003c m; i++){\n        cin \u003e\u003e s;\n        string newSignal = \"\";\n\n        for(int j = 0; j \u003c n; j++){\n            for(int x = 0; x \u003c k; x++){\n                newSignal.push_back(s[j]);\n            }\n        }\n        for(int y = 0; y \u003c k; y++){\n            signal.push_back(newSignal);\n        }\n    }\n    for(string s: signal){\n        cout \u003c\u003c s \u003c\u003c endl;\n    }\n    return 0;\n}\n\n```\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/comp/lostcow":{"title":"The Lost Cow","content":"\n\nUSACO 2017 US Open Contest, Bronze - Problem 1. The Lost Cow\n# Problem Statement\n\nFarmer John has lost his prize cow Bessie, and he needs to find her!\nFortunately, there is only one long path running across the farm, and Farmer John knows that Bessie has to be at some location on this path. If we think of the path as a number line, then Farmer John is currently at position $x$ and Bessie is currently at position $y$ (unknown to Farmer John). If Farmer John only knew where Bessie was located, he could walk directly to her, traveling a distance of $|x−y|$. Unfortunately, it is dark outside and Farmer John can't see anything. The only way he can find Bessie is to walk back and forth until he eventually reaches her position.\n\nTrying to figure out the best strategy for walking back and forth in his search, Farmer John consults the computer science research literature and is somewhat amused to find that this exact problem has not only been studied by computer scientists in the past, but that it is actually called the \"Lost Cow Problem\" (this is actually true!).\n\nThe recommended solution for Farmer John to find Bessie is to move to position $x+1$, then reverse direction and move to position $x−2$, then to position $x+4$, and so on, in a \"zig zag\" pattern, each step moving twice as far from his initial starting position as before. As he has read during his study of algorithms for solving the lost cow problem, this approach guarantees that he will at worst travel 9 times the direct distance $|x−y|$ between himself and Bessie before he finds her (this is also true, and the factor of 9 is actually the smallest such worst case guarantee any strategy can achieve).\n\nFarmer John is curious to verify this result. Given $x$ and $y$, please compute the total distance he will travel according to the zig-zag search strategy above until he finds Bessie.\n\n#### Input Format (cowsignal.in)\n\nThe single line of input contains two distinct space-separated integers $x$ and $y$. Both are in the range $0 \\ldots 1,000$.\n\n#### Output Format (cowsignal.out)\n\nPrint one line of output, containing the distance Farmer John will travel to reach Bessie.\n\n\n\n#### Sample Input: \n3 6\n#### Sample Output:\n\n9\n\n# My Solution\n\u003cbr\u003e\n\n```\n#include \u003ciostream\u003e\n#include \u003cvector\u003e\nusing namespace std;\n\nint main() {\n  freopen(\"lostcow.in\", \"r\", stdin);\n  freopen(\"lostcow.out\", \"w\", stdout);\n\n  int x, y;\n  scanf(\"%d %d\", \u0026x, \u0026y);\n  int origin = x;\n\n  int d = 0;\n  int k = 1;\n\n  // approach:\n  int turn = 1;\n  while (x != y) {\n    if (turn % 2 != 0) {\n      for (int i = 1; i \u003c= k; i++) {\n        if (x == y)\n          break;\n        x++;\n        d++;\n      }\n\n      for (int i = 1; i \u003c= k; i++) {\n        if (x == y)\n          break;\n        x--;\n        d++;\n      }\n\n    } else {\n      for (int i = 1; i \u003c= k; i++) {\n        if (x == y)\n          break;\n        x--;\n        d++;\n      }\n\n      for (int i = 1; i \u003c= k; i++) {\n        if (x == y)\n          break;\n        x++;\n        d++;\n      }\n    }\n    k *= 2;\n    turn++;\n  }\n\n  cout \u003c\u003c d \u003c\u003c endl;\n  return 0;\n}\n\n```\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/6.824/DistributedSystemsIntro":{"title":"Introduction to Distributed Systems","content":"\nThe core of a **distributed system** is a set of cooperating computers that are communicating with each other over a network to get some coherent task done. Examples are storage, big data computations, and peer-to-peer file sharing. A lot of critical infrastructure out there is built by distributed systems. If you're designing a system, if you can possibly solve it with a single computer, do that. It's **always** easier that way. Try everything else before a distributed system, because they're not simpler.\n\n## Reasons we use distributed systems\n1. Performance: How do I get 1000 computers to give me 1000x the compute?\n    - The reason people use lots of cooperating computers is because they need high-performance, and to accomplish that they need parallelism. \n2. Fault Tolerance\n    - Have two computers do the exact same thing, if one fails give the task to the other one.\n3. Physical\n    - Some problems are naturally spread out into space and become inherently physically distributed systems.\n    - Two Banks across the country dealing with the transfer of money.\n4. Security \n    - Improve security by splitting things between multiple computers, resulting in isolation.\n\n\n\n## Basic Challenges\n\n##### Concurrency \n\nBecause distributed systems compute concurrently, you get all the problems that come up with concurrent programming. Complex interactions and weird timing dependant stuff make the world a scary place.\n\n##### Partial Failure\n\nDistributed is hard because with all the different computers in addition to a network, you will have unexpected failure patterns. If you were working with a single computer, it's usually the case that the computer is working, or it's not. A distributed system may have multiple computers not working, or a single computer not working. A part of the network might be down. These result in partial failures.\n\n##### Performance\n\nThough we build computers to obtain high-performance, there's a lot of roadblocks in the way. It takes careful design to get the performance you feel you deserve.\n\n\n\n\n\n\n\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/6.824/FaultTolerance":{"title":"Fault Tolerance","content":"\nSee Also: [Scalability](Scalability.md), [Infrastructure for Applications](Infrastructure%20for%20Applications.md), and [Introduction to Distributed Systems](DistributedSystemsIntro.md)\n\nA single computer can often stay up for years. However, if you're building a system with thousands of computers, that means you're going to have about 3 computer failiures per day. Big problems with big distributed systems turn rare failiure problems to failiure problems that happen all the time. There's almost always something broken: a computer crashed, a piece of the network is down, someone stepped over a cable, a fan is overheating - something always goes wrong. \n\n# Designing around Fault Tolerance\n\nThis means that when you're designing a distributed system, you have to bake in the masking or invisibility of these constant failiures. This makes the programmer's life easier. \n\nSome systems are designed to continue providing, despite partial failiures, undamaged service. These systems are called **available**. The way available systems are specified is that they will stop working after a certain amount of partial failiure.\n\nOther systems will stop working when there are failiures and wait for the failed components to be repaired. But when it does recover, when those components are fixed, the system will continue providing service as if nothing ever went wrong. Systems like these need to do things like save their data on disc. Note available systems can also be recoverable.\n\n## Storage\n\nNon-volatile storage is a good way to keep track of the state of the system. These tend to be expensive to update. \n\nAnother tool is replication. The problem is that the two replicas tend to drift out of sync and stop becoming replicas. \n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/6.824/Infrastructure-for-Applications":{"title":"Infrastructure for Applications","content":"\nSee also: [Introduction to Distributed Systems](DistributedSystemsIntro.md)\n\nA lot of our goal is to discover **abstractions**, or ways of simplifying the interface of distributed storage and infrastructure so that it is easy to build applications on top of it. We want to *hide* the fact the system is distributed.\n\n# Implementation \n\n1. Remote Procedure Call (RPC)\n    - Mask the fact you're communicating over an unreliable network.\n\n2. Threads\n    - A way of structuring concurrent operations so that the programmer lives a happier life.\n\n3. Concurrency control\n    - We'll need to spend a certain amount of time thinking about this.\n\n\n\n\n\n\n\n\n\n\n\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/6.824/Scalability":{"title":"System Scalability","content":"\nUsually the high-level goal of building a distributed system is to build something with scalable speed-up. It's the idea that with 2 times the computer resources, I should get 2 times the throughput. The alternative is to pay programmers to restructure your software. To get more performance, you can either buy more compute, or better programmers. Compute is cheaper, so more ideal.\n\n## Got a website?\n\nWhen you host a website, it runs on a web server and has a database attached to it. To add more compute so that more users can visit your website, you add more web servers. This kind of scalability is rarely infinite, and as you add more web servers, suddenly the database has too much pressure. ","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/ADT":{"title":"Abstract Data Types","content":"\nAbstract Data types are defined by their operations, not their implementation.\n\n# The Stack ADT\nThe Stack ADT supports the following operations:\n- `push(x)`: Add an element to the top of the stack.\n- int pop(): Remove the top element from the stack and return it.\n\nA linked list and array implementation are basically equally efficient.\n\n# GrabBag ADT\n- `add(x)`: Add an element to the grab bag.\n- int `remove()`: Remove a random element from the grab bag and return it.\n- int `sample()`: Return a random element from the grab bag.\n- int `size()`: Return the number of elements in the grab bag.\n\nArrays turn out to be a better implementation than linked lists.\n\n    \n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/BSTheight":{"title":"BST Tree Height","content":"\nHeight varies dramatically between \"bushy\" and \"spindly\" trees.\n\n- Bushy trees have a height of roughly $log(n)$.\n- Spindly trees have a height of $n$.\n\nThis means that for a bushy tree, we have to double the number of nodes to increase the height by 1. For a spindly tree, we have to add 1 node to increase the height by 1. \n- Performance of operations on a spindly tree can be just as bad as a linked list.\n\nImportantly, Big-O and Theta notation are different. Big-O is less precise, and can be analogus to an inequality. If I say that a Binary Tree's height is $O(n^{2})$, what I'm saying in english is\n\n\u003e The height of a binary tree grows less than or equal to n^{2}\n\nThis is still true.\n\nWhereas, if I say a binary tree's height is $\\Theta(log(n))$, what I'm saying in english is\n\n\u003e The height of a binary trees grows equal to $log(n)$ in the best case ($n$ in the worst case).\n\nNote that Big-O is still useful. In the real world, Big-O is shorthand for \"in the worst case\". \n- It allows us to make simple blanket statements\n- Sometimes we don't know the exact runtime, so we use Big-O to give an upperbound\n- It can be easier to write proofs for Big-O than Big-Theta.\n\n\n\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/BStperformance":{"title":"BST Performance","content":"\nHeight and average depth are important properties of BSTs:\n- The depth of a node is how far it is from the root\n- The height of a tree is the depth of its deepest leaf\n- The average depth of a tree is the average depth of a tree's nodes.\n\n$$ \\text{Average Depth} = \\frac{\\text{Total Depth}}{\\text{Number of Nodes}} $$  \n\nAdding in linear order will result in a spindly tree. Adding in random order will result in a bushier tree.\n\nWe now know and have seen in great detail is that \n- the worst case is $\\Theta(n)$\n- the best case is $\\Theta(log(n))$\n\nRandom trees have an average depth of $\\Theta(log(n))$. In other words, random trees are bushy, not spindly.\n\n# Mathemetical Analysis\n\nThe average depth of a tree is $ ~2log(n) = \\Theta(log(n))$. The tilda (~) is can kind of be thought of as $\\Theta$ but we don't drop the constant. Thus the average runtime for `contains` is $\\Theta(log(n))$ on a tree built with random inserts.\n\nIf $N$ distinct keys are inserted into a BST in random order, the expected height of the tree is $~4.311log(n)$. This is beyond the scope of this text (the proof was made in 2003).\n\n# Bad news\n\nWe can't always insert our items in random order. This is because Data comes in overtime! We don't have it all in advance. If you're storing a bunch of dates, your binary tree will be horrible. \n\n\n\n\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/Btree":{"title":"B-Tree","content":"\nB-Trees are a kind of self-balancing BST. \n\n# B-Trees\n\nTo start, let's make a rule to never add a new leaf. Instead, we'll just add any new value to a node.\n\nBut this node can get quite big, and our data structure will degenerate to a linked list. That's no good. Instead, let's set a limit $L$ on the number of items\nthat can be stored in our node. \n\n- If a node has more than L items, give an item to parent.\n- Point parent to new node that was split off.\n\nExamining a node costs us $O(L)$ compares, but that's fine since L is constant.\n\n# What if we split non-leaf nodes?\n\n- Observation: Splitting-trees have perfect balance. \n  - If we split the root, every node gets pushed down a level.\n  - If we split an internal node, the height doesn't change. \n\n# Terminology\n\nSplitting Trees are B-Trees. B-Trees of order $L = 3$ are called 2-3-4 trees or 2-4 trees. \n- B-Trees trees of order $L = 2$ are called 2-3 trees. \n- The \"2-3-4\" refers to the number of children a node can have. For example, a 2-3-4 tree has 2, 3, or 4 children.\n\n\nNote that the origin of the name \"B-Tree\" is unclear. \"Broad\", \"Bushy\", and \"Balanced\" are all possible explanations. The most likely one is \"Bayer\", since the first paper on B-Trees was written by Bayer.\n\n# B-Trees in Practice\n\n- B-Trees are used in databases and file systems. L would be very large (in the thousands).\n- Conceptually simple balanced search tree.\n\n\n\n\n\n\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/bstoperations":{"title":"BST Operations","content":"\n# Search\n\nIf searchKey = T.key, return.\n- If searchKey \u003c T.key, search T.left;\n- If searchKey \u003e T.key, search T.right;\n\nRuntime to complete search on a bushy BST in the worst case is $O(log(n))$. This is intuitive to see and you can generally estimate the number of computations needed to search a BST by the height of the tree.\n\nRemember, the height of the tree is roughly $log(n)$. So if the tree is 15 nodes, $log(15) = 4$. So the worst case runtime is $O(4) = O(1)$.\n\nBushy BSTs are extremely fast.\n\nAt 1 microsecond per operation, can find something from a tree with $10^{300000}$ nodes in one second. \n\nMuch (perhaps most?) computation is dedicated towards finding things in response to queries. \n\n# Insert \n\nSearch for key. \n- If found, do nothing.\n- If not found \n  - Create a new node\n  - Set appropiate link. \n\n# Delete\n\n3 cases: \n\n- Deletion key has no children\n- Deletion key has one child\n- Deletion key has two children\n\n\nIn the first case, we just sever the link. It gets collected by the garbage collector.\nIn the second case, we replace the node with its child. Note that even though the deletion key might still be pointing to the child, it will still get collected by the garbage collector.\nIn the third case, we replace the node with its predecessor or the successor. Note that the predecessor or successor are guarenteed to have at most one child.\n\nThis is usually known as Hibbard deletion.\n\n# BSTs as Sets and Maps\n\n\nThe BST we created could be thought of as a set. But we can also use it as a map. This is done by storing each BST node as key/value pairs. Note that there is no efficient way to look up a value in a BST.\n- Example: Cannot find all the keys with value = 1 without iterating over ALL nodes. This is fine. \n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/btreeinvariant":{"title":"B-Tree Analysis","content":"\n# Bushiness and Invariants\nUnlike in BSTs, where if you insert things in order the tree will be spindly, in B-Trees, if you insert things in order, the tree will be bushy no matter what.\n\nWe get two nice invariants:\n- All leaves are the same distance from the source\n- A non-leaf node with $k$ items must have $k+1$ children\n\nThese invariants guarentee a bushy tree.\n\n# Runtime Analysis\n\nLet's analyze the runtime of a `contains` operation.\n- In the worst case, we're going to need to check $H+1$ nodes, where $H$ is the height of the tree.\n- In the worst case, we're going to need to check $L$ items in each node.\n\nThis means our runtime will be $O((H+1)L)$ which is $O(HL)$.\n\nHowever, we know that $H = \\Theta(log(n))$, so $O(HL) = O(Llog(n))$ = $O(log(n))$. Note L is a constant, so we can drop it.\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/graph":{"title":"Graphs","content":"\nTrees are fantastic for representing strict hierarchial relationships.\n- But not every relationship is hierarchial.\n\nA graph consists of:\n- a set of nodes\n- a set of zero or mor edges, which connects two nodes.\n\n# Simple Graphs\n\nA simple graph is a graph with:\n- no edges that connects a vertex to itself, i.e. no loops\n- No two edges that connect the same vertices, i.e. no parallel edges.\n  \nNote you can think think of edges as a pair of vertices.\n\nVertices with an edge between are called adjacent.\n\nVertices or edges may have **labels** or **weights**.\n\nA **path** is a sequence of vertices such that each pair of consecutive vertices is connected by an edge.\n\nA cycle is a path that starts and ends at the same vertex.\n  \nA graph with a cycle is called **cyclic**. A graph without a cycle is called **acyclic**.\n\nTwo vertices are **connected** if there is a path between them. If all vertices are connected, the graph is connected.\n\nThere is other terminology, like vertex degree and connected components.\n\n\n  ","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/graph-problems":{"title":"Graph Problems","content":"\n# Graph Queries\n\nThere are a lot of interesting questions we can ask about a graph. What is the shortest path between two points? Are there cycles? What is the longest path without cycles?\n\nIs there a path we can take that only uses each node exactly once? Is there a tour that uses each edge exactly once?\n\nWhat's interesting about these problems is that they're solved with traversals.\n\n# Well known Graph Problems\n\n- **s-t path problem**: Given a graph and two vertices s and t, is there a path from s to t? (GPS)\n- **Connectivity**: Is the graph connected? I.e. is there a path between all vertices? (A generalization of the s-t path problem)\n- **Bi-connectivity**: Is there a vertex whose removal would disconnect the graph? (Biconnectivity might be a bad because you'd have a single point of failure)\n- **Shortest s-t path**: Given a graph and two vertices s and t, what is the shortest path from s to t? (Google Maps)\n- **Cycle detection**: Is there a cycle in the graph?\n- **Euler Tour**: Is there a cycle that uses each edge exactly once?\n- **Hamiltonian Tour**: Is there a cycle that uses each vertex exactly once?\n- **Planarity**: Can you draw the graph on paper with no crossing edges?\n- **Isomorphism**: Are two graphs the same? \n\nWe often can't tell how difficult a graph problem is without very deep consideration. \n\nAn efficient Euler tour algorithm O(# edges) was found as early as 1873. But despite decades of intense study, no efficient algorithm for a Hamilton tour exists. Best algorithms are exponential time.\n\nGraph problems are among the most mathemtically rich areas of CS theory.\n\n\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/hash/hash-collisions":{"title":"Hashing Collisions","content":"\nPigeonhole principle tells us that collisions are inevitable due to integer overflow.\n- hash code for \"moo\" and \"nep\" might both be 718.\n\nThe way we deal with this is creating \"buckets\" where a collision may occur. This can look like anything; linked lists, arraylists, etc. \n\nThat means the runtime for `contains` and `insert` will be $O(Q)$ where $Q$ is the length of the longest list. \n\nOne more innovation: We don't need billions of buckets or indexs. We can reduce them. For example, if our hash code is 123109, we can apply `% 10` to get 9.\n\nThis is basically what a hash table is.\n\n# The pipeline\n\nWe start with something we want to store, say the word 'hi', inside the hash table. We then compute the hash code using a hash function. We reduce the hash code in some way, and store it at that index. \n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/hash/hash-intro":{"title":"Hashing Intro","content":"\nOur search trees have excellent $O(log(n))$ performance. Can we do better? And can we do it without comparisons?\n\n# Approach: Taking advantage of arrays\n\nThe idea is simple: Create a boolean array of $N$ length that is initialized as false. Whenever we add an element, we set the corresponding index to true. \n\nThis makes `contains` and `add` O(1).\n\nThe draw back is that we use a ridiculous amount of memory, and we don't yet have a way to generalize data types. \n\n# ASCII and Unicode\n\nThe ASCII Standard is the most basic character set used by computers. It is a 7-bit character set, which means it can represent 128 different characters.\n\nIf we want to represent more characters, we can use Unicode. Unicode is a 16-bit character set, which means it can represent 65,536 different characters. This would include things like Chinese characters, emojis, and other symbols.\n\nWe can use the ASCII and Unicode character sets to create a hash function.\n\n# Integer Overflow\n\nIn Java, the largest possible Integer is 2,147,483,647. If we add 1 to this number, we get -2,147,483,648. This is called integer overflow, meaning we start back at the smallest possible integer.\n\nThis means that if we represent words in ASCII, which is base 128, we can only represent 2,147,483,647 words. Which means we can get collisions. \n\nTHe term we use is 'hash code' to refer to the integer that we get from our hash function. Wolfram Alpha says that a hash code \"projects a value from a set with many (or even infinite) members to a value from a set with a fixed (fewer) number of members.\"\n\nThe pigeonhole principle tells that we will get collisions. It is inevitable.\n\nThat leaves us with two fundamental challenges:\n\n1. How do we avoid collisions?\n2. How do we compute the hash code for arbitrary objects?\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/hash/hash-performance":{"title":"Hash Performance","content":"\nUnfortunately, the hash table we devised is $O(Q)$ for its operations, and $Q$ has $O(N)$ order of growth.      \n\nThis is really bad, because eventually our operations will get slow.\n\nWe can fix this by adding more buckets!\n\n# Model\n\nSuppose we have:\n- M buckets\n- An increasing number of $N$ items.\n\nAn example strategy: When $\\frac{N}{M}$ is $\\geq$ 1.5, then double M.\n- $\\frac{N}{M}$ is known as the `load factor`. It represents how full the hash table is. \n\nAs long as $M = \\Theta(N)$, then $N = \\Theta(\\frac{N}{M}) = O(1)$.\n\nOne thing to note is that when we do our resizing, items in our table will shuffle around and the lists will get shorter. \n- Resizing takes $O(N)$ time.\n- Most `add` operations will take $O(1)$ time. Some will take $O(N)$ time to resize.\n-  Also, the specific load factor constant doesn't matter. We will still get $O(1)$ time.\n\nMore specifically, with resizing, `contains` ia $\\Theta(1)^{†}$ and `add` is $\\Theta(1)^{* †}$. `†` means that we assume items are evenly spread, and `*` means \"on average\".\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/heap":{"title":"Heaps","content":"\nHeaps are a pretty famous data structure. Note that they're not at all related to 'the heap', something you'd learn in an operating systems class.\n\n# Introducing the Heap\n\nIt is a binary tree with the following properties:\n- It is a complete binary tree (all levels are filled except the last, which is filled from left to right)\n- It is a heap-ordered binary tree (the key in each node is larger than the keys in its children)\n\n# Heap Operations\n\n## Insert\n\n- Add the new node to the bottom level of the heap, as far left as possible\n- Swim the new node up until it is larger than its parent, or it is the root\n\n## Delete Smallest\n\n- Swap the root with the node at the bottom level, farthest right\n- Sink the new root down until it is smaller than both of its children, or it is a leaf\n\n## Get Smallest\n- Return the root","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/pq":{"title":"Priority Queue","content":"\nA priority queue allows tracking and removal of the smallest item.\n\nthe interface;\n- `add`\n- `getSmallest`\n- `removeSmallest`\n\nIt can transform a problem which requires a lot of memory into one which requires much less. \n\n# Implementation\n\nWe could use an ordered array, but adding and removing elements would be $O(N)$. We could also use a bushy BST, and for add, removr, and getSmallest, we would have $O(\\log N)$ time. The issue is the possibility of duplicates, which is very awkward to implement/fix (though possible!). We could also use a hash map, and adding would be $O(1)$, but removing and getting the smallest would be $O(N)$ because we don't know what bucket the smallest item is in.\n\nSo what we need is a heap. It will give us $O(\\log N)$ for all operations, and it will also allow duplicates.\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/red-black-tree":{"title":"Red Black Trees Intro","content":"\n\u003e \"Beautiful algorithms are, unfortunately, not always the most useful.\" - Donald Knuth\n\nB-Trees are painful to implement. And even if you do implement them, they're still not that great. They're $O(log(n))$, but there are better $O(log(n))$ data structures out there.\n\nSome of the things that make it painful:\n\n- Maintaining different node types.\n- Interconversion of nodes between 2-nodes and 3-nodes\n- Walking up the tree to split nodes.\n\n# The idea\n\nIn a normal BST, we can get different BSTs depending on our insertion order. \n- In general, for $N$ items, there are [$\\textnormal{Catalan}(N)$](https://en.wikipedia.org/wiki/Catalan_number) different BSTs.\n\nBut given any BST, we can move to a different configuration using a rotation. \n\nRotation definition: A rotation is a transformation of a tree that preserves the BST property.\nExample: A left rotation on a node $x$ is a transformation that moves $x$'s right child to $x$'s position, and moves $x$ to $x$'s right child's position. \n\nWe can shorten (or lengthen) a BST by performing a rotation. There is a paper proving we can do an optimal set of rotations in $O(n)$. \n \n\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/red-black-tree-def":{"title":"Red Black Trees Definition","content":"\n\n Red Black Trees are trees that are structurally indentical to B-Trees, but have glue links. This idea is commonly used in practice  (ex: `java.util.TreeSet`).\n\n A BST with left glue links are called a **left-leaning red-black BST**. \n - LLRBs are normal BSTs.\n - There is a 1-1 correspondence between an LLRB and an equivalent 2-3 tree. \n - The red is just convenient fiction. Red links don't \"do\" anyything special.\n\n# Red Black Tree Properties\n\nFrom this, we get two very important properties:\n- No node has two red links connected to it.\n- Every path from a leaf node to the root has the same number of black links.\n\nTherefore, LLRBs are **perfectly balanced**.\n\nThe way we'd implement this is to insert as usual into a BST, and then do some number of rotations to maintain the 1-1 mapping. \n\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/redblacksummary":{"title":"Red Black Trees Performance","content":"\n# LLRB Runtime\n\n- LLRB has height $O(log(n))$\n- Contains is trivially $O(log(n))$\n- Insert is $O(log(n))$\n  - $O(log(n))$ to add new node\n  - $O(log(n))$ to fix up tree (rotation and color flips)\n\nWe won't discuss delete because it's uninteresting.\n\n#  Summary\n\n- BSTs are simple, but they are subject to imbalance.\n- 2-3 Trees (B-Trees) are balanced, but are painful to implement and are slow.\n- LLRBs insertion is simple to implement (but delete is hard).\n  - Works by maintaining a mathematical bijection with 2-3 trees.\n- Java's TreeMap is a red-black tree (not left leaning).\n - Maintains correspondence with a 2-3-4 tree (is not a 1-1 correspondence).\n - Allows glue links on either side\n - More complex implementation, but significantly (?) faster.\n\n# Footnote\n\nThere are many other types of search trees out there. AVL trees, splay trees, treaps, etc. \n- And there are other efficient ways to implement sets and maps entirely.\n    - Skip lists\n    - Hashing (the most common alternatives)\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/rrbinsert":{"title":"Red Black Trees Rules","content":"\n# Red Black Tree Rules\n\n\nWhen inserting: use a red link\n- If there is a right leaning \"3-node\", we have a left leaning violation.\n - Rotate left \n- If there are two consecutive left links, we have an incorrect 4-node violation\n    - Rotate right\n- If a node has two red link children, we have a temporary 4-node violation\n    - Flip colors\n\n\nOne last detail: Cascading operations\n- It is possible that a rotation or flip will cause an additional violation that needs fixing.\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/tree-traversal":{"title":"Tree Traversals","content":"\nUnlike a linked list, where you can traverse the list by following the `next` pointers, a tree is a bit more complicated. There are more ways to traverse a tree.\n\n## Level Order Traversal\n- Visit top-down, left to right (like reading in English)\n\n## Depth First Traversals\n\nThere are three types of depth first traversals:\n- Preorder\n- Inorder\n- Postorder\n\nThe basic idea is to traverse \"deep nodes\" before shallow ones. Note that traversing a node is different than \"visiting\" a node. \n\n## Preorder\n\nA PreOrder traversal is a depth first traversal where the root is visited first, then the left subtree, then the right subtree. \n  \n``` \npreOrder(BSTNode x){\n    if(x == null) return;\n    print(x.key);\n    preOrder(x.left);\n    preOrder(x.right);\n}\n```\n\n## Inorder\n\nA InOrder traversal is a depth first traversal where the left subtree is visited first, then the root, then the right subtree. \n\n```\ninOrder(BSTNode x){\n    if(x == null) return;\n    inOrder(x.left);\n    print(x.key);\n    inOrder(x.right);\n}\n```\n\n## Postorder\n\nA PostOrder traversal is a depth first traversal where the left subtree is visited first, then the right subtree, then the root. \n\n```\npostOrder(BSTNode x){\n    if(x == null) return;\n    postOrder(x.left);\n    postOrder(x.right);\n    print(x.key);\n}\n```\n\n# Handy Trick\n\nTo visualize a traversal, you can do the following:\n\nTrace a path around the graph, from top going counter-clickwise.\n- Preorder: Visit node everytime we pass the left of a node\n- Inorder: Visit node everytime we pass the bottom of a node\n- Postorder: Visit node everytime we pass the right of a node","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/data-structures/trees":{"title":"Trees","content":"\n\nA tree consists of \n- A set of nodes\n- A set of edges that connect those nodes\n  - Constraint: There is exactly one path between any two nodes.\n\nIn a rooted tree, we call one node priviledged node the root.\n- Every node N except the root has exactly one parent, deifned as the first node on the path from N to the root.\n- The root is usually depicted as tthe top of the tree (kind of backwards from how we usually think about trees)\n- A node with no child is called a leaf.\n\nIn a rooted binary tree, every node has either 0, 1, or 2 children (subtrees).\n\nA binary search tree is a rooted binary tree with one additional property: the BST property.\n\nBST property: For every node X in the tree: \n- Every key in the left subtree is less than X's key\n- Every key in the right subtree is greater than X's key\n\nNote that there is a difference between a Binary Tree, and a binary search tree. A binary tree is a tree where every node has at most two children. A binary search tree is a binary tree where the BST property holds.\n\nOrdering must be complete, transitive, and antisymmetric. Given keys p and q:\n- Exactly one p \u003c q or q \u003c p are true.\n- if p \u003c q and q \u003e r, then that implies p \u003c r  \n\nOne consequence of these rules : No duplicate keys are allowed in a BST.\n- keeps things simple. Most real world follow this rule.\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/ds/introdb":{"title":"Introduction to Databases","content":"\nIn a very long sentence, Database Management Systems (DBMS) provides efficient, reliable, convenient, and safe multi-user storage of and access to massive amounts of persistent data.\n\n# Massive\n\nDatabase systems are handling terabytes of data and more. This is a lot of data, and it's growing every day. This is much more data than a single computer can handle. \n\n# Persistent\n\nData is typically persistent, meaning that the data in the database outlives the programs that execute on that data. By contrast, data in a program is typically volatile, meaning that the data in the program is lost when the program terminates.\n\n# Safety\n\nDatabase systems, since they provide critical applications like banking and telecommunication, must be safe. There has to be a guarentee that the data in the system won't be lost. There can be hardware failiures, software failiures, power outages, malicious users, etc. So there have to be mechanisms to ensure that the data is safe.\n\n# Multi-user\n\nDatabase systems are used by multiple users at the same time. This means that the database system has to provide concurrency control, which is the idea that multiple users can access the database at the same time without corrupting the data. We also don't want a single user having exclusive access to the database, because that would be cause performance to slow down considerably. \n\n# Convenient\n\nDatabase systems are designed to be easy to work with large amounts of data, and do powerful and interesting things with that data. There are a couple levels of which that happens:\n\n- Physical data independence - the physical storage of the data is hidden from the user. The user doesn't have to worry about how the data is stored, they just have to worry about how to access the data.\n\n- High-level query languages (declarative) - the user doesn't have to worry about how to access the data, they just have to worry about what data they want.\n\n- Effiency - databases have to do thousands of queries/updates per second. This means that the design of databases is not easy, and there are a lot of optimizations that go into making databases fast.\n\n- Reliable - 99.9999% uptime is the goal. This means that the database has to be able to recover from hardware/software failures, and it has to be able to recover from user errors.\n\n\n\n\n ","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/notes/life/minimal":{"title":"Minimalistic Workout","content":"\n# Full Body Day 1\n\nWarm up: Brisk Walking and Dynamic Stretches\n\n1. Flat Dumbbell Press: 1x4-6 reps heavy 1x 8-10 reps backoff weight\n2. Dumbbell Romanian Deadlift: 3x8-10\n3. 2-grip Lat Pulldown: 2x10-12 reps\n4. Dumbbell Step Up: 1x8-10\n5. Overheap Cable Triceps Extension 1x12-15 reps + Dropset\n6. Machine Lateral Raise: 1x12-15 reps\n7. Leg press toe raise 1x12-15 reps\n\n\n# Full Body Day 2\n\nWarm up: Brisk Walking and Dynamic Stretches\n\n1. Hack Squat: 1x4-6 reps + 1x8-10 reps\n2. Antagonistic Superset\n    - T-bar row: 2x10-12\n    -  High Incline Smith Press: 2x10-12\n3. Replace with dumbbell\n4. Seated Leg Curl: 1x10-12\n5. EZ-Bar Bicep Curl: 1x12-15\n6. Cable Crunch: 1x12-15\n\n\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/private/ds/bstoperations":{"title":"BST Operations","content":"\n# Search\n\nIf searchKey = T.key, return.\n- If searchKey \u003c T.key, search T.left;\n- If searchKey \u003e T.key, search T.right;\n\nRuntime to complete search on a bushy BST in the worst case is $O(log_2(n))$. This is intuitive to see and you can generally estimate the number of computations needed to search a BST by the height of the tree.\\\n\nRemember, the height of the tree is roughly $log_2(n)$. So if the tree is 15 nodes, $log_2(15) = 4$. So the worst case runtime is $O(4) = O(1)$.\n\nBushy BSTs are extremely fast.\n\nAt 1 microsecond per operation, can find something from a tree with $10^300000$ in one second. \n\nMuch (perhaps most?) computation is dedicated towards finding things in response to queries. \n\n# Insert \n\nSearch for key. \n- If found, do nothing.\n- If not found \n  - Create a new node\n  - Set appropiate link. \n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/private/ds/disjointsets":{"title":"Disjoint Sets","content":"\n\n```\nclass DisjointSet {\n    ArrayList\u003cArrayList\u003cInteger\u003e\u003e nums = new ArrayList\u003cArrayList\u003cInteger\u003e\u003e();\n    \n    public DisjointSet(int length){\n        for(int i = 0; i \u003c length; i++){\n            ArrayList\u003cInteger\u003e num = new ArrayList\u003cInteger\u003e();\n            num.add(i);\n            nums.add(num);\n        }\n    }\n    \n    \n    public void printSet(){\n        System.out.print(\"{\");\n        for(int i = 0; i \u003c nums.size(); i++){\n            System.out.print(\"{\");\n            for(int j = 0; j \u003c nums.get(i).size(); j++){\n                if(j+1 \u003e= nums.get(i).size()){\n                    System.out.print(nums.get(i).get(j));\n                } else {\n                    System.out.print(nums.get(i).get(j) + \",\");\n                }\n            }\n            System.out.print(\"}\");\n        }\n        System.out.println(\"}\");\n    }\n    \n    public void connect(int x, int y){\n        for(int i = 0; i \u003c nums.size(); i++){\n            if(nums.get(i).contains(x)){\n                nums.get(i).add(y);\n                i++;\n            } else if (nums.get(i).contains(y)){\n                nums.remove(i);\n            }\n        }    \n    }\n    \n    public boolean isConnected(int x, int y){\n         for(int i = 0; i \u003c nums.size(); i++){\n             if(nums.get(i).contains(x) \u0026\u0026 nums.get(i).contains(y)){\n                 return true;\n             }\n         }\n         return false;\n    }\n    \n\n    \n    public static void main(String[] args){\n        DisjointSet set = new DisjointSet(10);\n        set.printSet();\n        set.connect(1,3);\n        set.printSet();\n        System.out.println(set.isConnected(4,6));\n        set.connect(3,7);\n        set.printSet();\n        System.out.println(set.isConnected(7,1));\n        set.printSet();\n        set.connect(2,7);\n        set.printSet();\n    }\n    \n}\n```","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/thoughts/netflow":{"title":"A Brief Overview of Netflow","content":"\nOne of the challenges of managing a network is understanding what's actually going through that network. We have a lot of different protocols buzzing around - HTTPS, SMTP, SFTP, DNS, etc - so how do we answer questions like how much of each protocol is happening? Who are the top talkers? What are the most visited destinations? \n\n# Lights, Camera, Action!\nThis is where Netflow improves our mortal lives. If you want to monitor your network traffic and see if there are any anomalies - worry no further. If you want to just see what your traffic patterns look like - with netflow it's trivial. \n\nThere are three basic ingredients:\n- A monitor that looks at network traffic as it passes in and out of an unsuspecting router. This traffic is cached in memory for a short period of time.\n- An exporter that takes the recorded traffic data and puts it into a network management system\n- A collector that runs on the network management system to do things like visualize the data.  \n\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null},"/thoughts/rereading":{"title":"Re-Reading","content":"\nFrom time to time I open books that I've already crawled through and squeezed once more. It often feels like a conversation with an old friend: the hurried catch-up, the gradual memories spurring from a long time past, the way their charisma and language just hit you and you're questioning what you're doing in life. When I look for inspiration to write - to storytell - I always go back to Paul Kalanithi's *When Breath Becomes Air*. \n\n# Memorable Sentences\n\n\u003e Even working on the dead, with their faces covered, their names a mystery, you find that their humanity pops up at you —in opening\n\u003e my cadaver's stomach, I found two undigested morphine pills, meaning that he had died in pain, perhaps alone and fumbling with the cap of a pill\n\u003e bottle. — pg. 47\n\n\u003e Once while showing us the ruin's of our donor's pancreatic cancer, the professor asked, \"How old is this fellow?\" \u003cbr\u003e\n\u003e \"Seventy-four,\" we replied. \u003cbr\u003e\n\u003e \"That's my age,\" he said, set down the probe, and walked away — pg. 50\n\n\u003e The secret is to know that the deck is stacked, that you will lose, that your hands or judgement will slip, and yet still struggle to win for \n\u003e your patients. You can't ever reach perfection, but you can believe in an asymptote toward which you are creaselessly striving.\n\n","lastmodified":"2023-01-01T02:55:41.720120086Z","tags":null}}