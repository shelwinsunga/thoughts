{"/":{"title":"Notes","content":"\nA catalog of notes, thoughts, and other digital type that I have created or consumed.\n\n\n### [† MIT 6.824: Distributed Systems](tags/6.824)\n### [CS61B: Data Structures and Algorithms](tags/Data-Structures)\n### [Stats 110: Probability](tags/Stats-110)\n### [CMU 15-445/645: Database Systems](tags/Databases)\n\n\n\n\n","lastmodified":"2023-04-04T19:05:28.95757793Z","tags":null},"/comp/cowsignal":{"title":"Cow Signal","content":"\nUSACO 2016 December Contest, Bronze - Problem 3. The Cow-Signal\n# Problem Statement\n\nBessie and her cow friends are playing as their favorite cow superheroes. Of course, everyone knows that any self-respecting superhero needs a signal to call them to action. Bessie has drawn a special signal on a sheet of $M×N$ paper $(1≤M≤10,1≤N≤10)$, but this is too small, much too small! Bessie wants to amplify the signal so it is exactly $K$ times bigger $(1≤K≤10)$ in each direction.\n\n#### Input Format (cowsignal.in)\n\nThe first line of input contains $M$,$N$, and $K$, separated by spaces.\nThe next $M$ lines each contain a length-$N$ string, collectively describing the picture of the signal.\n\n#### Output Format (cowsignal.out)\n\nYou should output $KM$ lines, each with $KN$ characters, giving a picture of the enlarged signal.\n\n\n#### Sample Input: \n5 4 2   \u003cbr\u003e\nXXX.    \u003cbr\u003e\nX..X    \u003cbr\u003e\nXXX.    \u003cbr\u003e\nX..X    \u003cbr\u003e\nXXX.    \u003cbr\u003e\n#### Sample Output:\n\nXXXXXX..    \u003cbr\u003e\nXXXXXX..    \u003cbr\u003e\nXX....XX    \u003cbr\u003e\nXX....XX    \u003cbr\u003e\nXXXXXX..    \u003cbr\u003e\nXXXXXX..    \u003cbr\u003e\nXX....XX    \u003cbr\u003e\nXX....XX    \u003cbr\u003e\nXXXXXX..    \u003cbr\u003e\nXXXXXX..    \u003cbr\u003e\n\n# My Solution\n\u003cbr\u003e\n\n```\n#include \u003ciostream\u003e\n#include \u003cvector\u003e\n#include \u003cstring\u003e\nusing namespace std;\n\nint main()\n{\n    freopen(\"cowsignal.in\", \"r\", stdin);\n\tfreopen(\"cowsignal.out\", \"w\", stdout);\n    int m,n,k;\n    scanf(\"%d %d %d\", \u0026m, \u0026n, \u0026k);\n    string s; \n    vector\u003cstring\u003e signal;\n    for(int i = 0; i \u003c m; i++){\n        cin \u003e\u003e s;\n        string newSignal = \"\";\n\n        for(int j = 0; j \u003c n; j++){\n            for(int x = 0; x \u003c k; x++){\n                newSignal.push_back(s[j]);\n            }\n        }\n        for(int y = 0; y \u003c k; y++){\n            signal.push_back(newSignal);\n        }\n    }\n    for(string s: signal){\n        cout \u003c\u003c s \u003c\u003c endl;\n    }\n    return 0;\n}\n\n```\n","lastmodified":"2023-04-04T19:05:28.95757793Z","tags":null},"/comp/lostcow":{"title":"The Lost Cow","content":"\n\nUSACO 2017 US Open Contest, Bronze - Problem 1. The Lost Cow\n# Problem Statement\n\nFarmer John has lost his prize cow Bessie, and he needs to find her!\nFortunately, there is only one long path running across the farm, and Farmer John knows that Bessie has to be at some location on this path. If we think of the path as a number line, then Farmer John is currently at position $x$ and Bessie is currently at position $y$ (unknown to Farmer John). If Farmer John only knew where Bessie was located, he could walk directly to her, traveling a distance of $|x−y|$. Unfortunately, it is dark outside and Farmer John can't see anything. The only way he can find Bessie is to walk back and forth until he eventually reaches her position.\n\nTrying to figure out the best strategy for walking back and forth in his search, Farmer John consults the computer science research literature and is somewhat amused to find that this exact problem has not only been studied by computer scientists in the past, but that it is actually called the \"Lost Cow Problem\" (this is actually true!).\n\nThe recommended solution for Farmer John to find Bessie is to move to position $x+1$, then reverse direction and move to position $x−2$, then to position $x+4$, and so on, in a \"zig zag\" pattern, each step moving twice as far from his initial starting position as before. As he has read during his study of algorithms for solving the lost cow problem, this approach guarantees that he will at worst travel 9 times the direct distance $|x−y|$ between himself and Bessie before he finds her (this is also true, and the factor of 9 is actually the smallest such worst case guarantee any strategy can achieve).\n\nFarmer John is curious to verify this result. Given $x$ and $y$, please compute the total distance he will travel according to the zig-zag search strategy above until he finds Bessie.\n\n#### Input Format (cowsignal.in)\n\nThe single line of input contains two distinct space-separated integers $x$ and $y$. Both are in the range $0 \\ldots 1,000$.\n\n#### Output Format (cowsignal.out)\n\nPrint one line of output, containing the distance Farmer John will travel to reach Bessie.\n\n\n\n#### Sample Input: \n3 6\n#### Sample Output:\n\n9\n\n# My Solution\n\u003cbr\u003e\n\n```\n#include \u003ciostream\u003e\n#include \u003cvector\u003e\nusing namespace std;\n\nint main() {\n  freopen(\"lostcow.in\", \"r\", stdin);\n  freopen(\"lostcow.out\", \"w\", stdout);\n\n  int x, y;\n  scanf(\"%d %d\", \u0026x, \u0026y);\n  int origin = x;\n\n  int d = 0;\n  int k = 1;\n\n  // approach:\n  int turn = 1;\n  while (x != y) {\n    if (turn % 2 != 0) {\n      for (int i = 1; i \u003c= k; i++) {\n        if (x == y)\n          break;\n        x++;\n        d++;\n      }\n\n      for (int i = 1; i \u003c= k; i++) {\n        if (x == y)\n          break;\n        x--;\n        d++;\n      }\n\n    } else {\n      for (int i = 1; i \u003c= k; i++) {\n        if (x == y)\n          break;\n        x--;\n        d++;\n      }\n\n      for (int i = 1; i \u003c= k; i++) {\n        if (x == y)\n          break;\n        x++;\n        d++;\n      }\n    }\n    k *= 2;\n    turn++;\n  }\n\n  cout \u003c\u003c d \u003c\u003c endl;\n  return 0;\n}\n\n```\n","lastmodified":"2023-04-04T19:05:28.95757793Z","tags":null},"/notes/110/110-lec1":{"title":"Probability and Counting","content":"\nStatistics is the logic of uncertainty. \n\n## Sample Space\n\nA **sample space** is the set of all possible outcomes of an experiment. You should interpret the word experiment extremely broadly. \n\nAn **event** is a subset of the sample space. \n\nOne of the big breakthrough's that made probability a mathematical subject instead of something like astrology was the idea of using **sets**. We'll need to be familiar with unions, intersections, etc.\n\n## Isaac Newton\n\nThere were gamblers asking Newton gambling questions because at the time, no one knew probability.\n\nNewton did the calculations correctly, but sometimes his intuition was wrong. Similarly, we'll be doing extremely unintuitive things in this class.\n\n\u003e In a Calculus class.. I've never seen anyone shocked, by anything. \n\nStatistics is full of counter-intuitive stuff, so that's why we need to make it more mathematical (set theory).\n\n## Naive Definition of Probability (very naive)\n\nYou can only use this definition when you have strong justification for doing so.\n\n**Definition:** $P(A) = \\dfrac{ \\textnormal{\\\\# Favorable Outcomes}}{\\textnormal{\\\\# Possible outcomes}}$\n\nThis means that the probability of $A$ is the number of favorable outcomes divided by the size of the sample space.\n\nThis definition makes the huge assumption that \n1. All outcomes are equally likely\n2. We have a finite simple space\n\nThis is a reasonable assumption in some problems with symmetry (coin flips, rolling dice). \n\n## Life on Neptune\nBut what if we asked: What's the probablity of life on neptune? \n\nUsing the naive definition we'd get an answer of $\\dfrac{1}{2}$.\n\nMoreover, what if we asked: What's the probability of intelligent life on neptune?\n\nStill, we'd get $\\dfrac{1}{2}$. But there's something weird about that, notably because you'd think that the probability is strictly less likely that there is intelligent life than if there was life at all!\n\n## Counting\n\nWe'll need to learn how to count.\n\n### Multiplication Rule\n\n\u003eIf we have an experiment with $n_1$ possible outcomes, and for each outcome of the 1st experiment there are tShen $n_2$ outcomes for the 2nd experiment, $\\cdots$, for each $n_r$\noutcomes for $r$ experiments, then $n_1 n_2 \\cdots  n_r$ overall possible outcomes.\n\n**Ice Cream:** Suppose you walk into an ice cream shop and need to choose between 2 kinds of cones (cake and waffle), and then 3 types of flavors (chocolate, vanilla, strawberry). How many possible outcomes are there?\n\nUsing our definition, we'll see that there are 2 outcomes for $n_1$ (cake or waffle), and 3 outcomes for $n_2$ (chocolate, vanilla, strawberry). Therefore the number of possible outcomes is $n_1 \\cdot n_2 = (2) * (3) = 6$. There are 6 possible outcomes. Note you can draw this in a tree structure.\n\nNote that the number of outcomes grow very fast! That's why it's impossible to hope to count them by hand except for the simplest problems. \n\n### Find the probability of a full house in poker\n\nA standard deck of cards contains 52 cards, we're assuming the deck is shuffled, and you get a 5 card hand.\n\nWhat's the number of possible hands? It's\n\n$${{52}\\choose{5}}$$\n\nSee [Binomial Coefficient](bincoeff.md) if you're not familiar with this notation.\n\nA full house is a hand with 3 cards of the same rank and 2 cards of the same rank. For example, 3 aces and 2 kings.\n\nSo the number of favorable outcomes is the number of ways to choose 3 cards of the same rank, times the number of ways to choose 2 cards of the same rank.\n\n$$\\dfrac{13\\cdot{{4}\\choose{3}} \\cdot 12 \\cdot {{4}\\choose{2}} }{{52}\\choose{5}}$$\n\nPrefer this tree structured way of thinking about it.\n\n\n## Sampling\n\nSampling means we have some population, and we want to draw a sample from it.\n\nWe're choosing $k$ elements from a population of size $n$, and we're wondering how many ways there are to do this.\n\nWe either sample with replacement (can we choose the same element twice?) or without replacement.\nAnd we either care about the order of the elements or don't.\n\n| | Order Matters | Order Doesn't Matter | \n|---|---|---|\n| With Replacement | $$n^k$$ | $${n+k-1}\\choose{k}$$|\n| Without Replacement | $$n(n-1)(n-2)\\cdots(n-k+1)$$ | $${n}\\choose{k}$$ |\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","lastmodified":"2023-04-04T19:05:28.95757793Z","tags":null},"/notes/110/bincoeff":{"title":"Binomial Coefficient","content":"\n$${n \\choose k} = \\dfrac{n!}{(n-k)!k!}$$\n\n\nIn english, you can think of this statement as saying \n\n\u003eIf we have $n$ people, how many ways can we choose $k$ people out of the $n$ people?\n\n## Derivation \n\n**Definitions:** Number of subsets of size $k$, of group $n$ people, where order doesn't matter.\n\n\nIf we had $n$ people and wanna select $k$ of them, here's how we'd start.\n\nLet's pick the first person. There are $n$ choices, so our expression looks like this:\n\n$$ n $$\n\nLet's pick the second person. There are $n-1$ choices left, since we already picked one.\n\n$$ n (n -1) $$\n\nSame thing for the third one\n\n$$ n (n - 1) (n - 2) $$\n\nThis generalizes \n\n$$ n (n - 1) (n - 2) \\cdots $$\n\nWe stop at $(n-k+1)$ because if $k = 1$, we want to stop at n. If $k = 2$, we want to stop at $n-1$, etc.\n\n$$ n (n - 1) (n - 2) \\cdots (n - k + 1) $$\n\nThis would be correct if we were picking people in a specific order. But in this case, we could've selected these people in any order. \n\nWe have to divide by $k!$ because there are $k!$ ways to order the people. \n\n$$ \\dfrac{n (n - 1) (n - 2) \\cdots (n - k + 1)}{k!} $$\n\nIt turns out this is the same as\n\n$$ \\dfrac{n!}{(n-k)!k!} $$\n\nNote that if $k \u003e n$ then ${n \\choose k} = 0$ because it wouldn't make sense to pick 11 people out of 10 people.\n\n\n\n\n\n","lastmodified":"2023-04-04T19:05:28.95757793Z","tags":null},"/notes/6.824/DistributedSystemsIntro":{"title":"Introduction to Distributed Systems","content":"\nThe core of a **distributed system** is a set of cooperating computers that are communicating with each other over a network to get some coherent task done. Examples are storage, big data computations, and peer-to-peer file sharing. A lot of critical infrastructure out there is built by distributed systems. If you're designing a system, if you can possibly solve it with a single computer, do that. It's **always** easier that way. Try everything else before a distributed system, because they're not simpler.\n\n## Reasons we use distributed systems\n1. Performance: How do I get 1000 computers to give me 1000x the compute?\n    - The reason people use lots of cooperating computers is because they need high-performance, and to accomplish that they need parallelism. \n2. Fault Tolerance\n    - Have two computers do the exact same thing, if one fails give the task to the other one.\n3. Physical\n    - Some problems are naturally spread out into space and become inherently physically distributed systems.\n    - Two Banks across the country dealing with the transfer of money.\n4. Security \n    - Improve security by splitting things between multiple computers, resulting in isolation.\n\n\n\n## Basic Challenges\n\n##### Concurrency \n\nBecause distributed systems compute concurrently, you get all the problems that come up with concurrent programming. Complex interactions and weird timing dependant stuff make the world a scary place.\n\n##### Partial Failure\n\nDistributed is hard because with all the different computers in addition to a network, you will have unexpected failure patterns. If you were working with a single computer, it's usually the case that the computer is working, or it's not. A distributed system may have multiple computers not working, or a single computer not working. A part of the network might be down. These result in partial failures.\n\n##### Performance\n\nThough we build computers to obtain high-performance, there's a lot of roadblocks in the way. It takes careful design to get the performance you feel you deserve.\n\n\n\n\n\n\n\n\n","lastmodified":"2023-04-04T19:05:28.95757793Z","tags":null},"/notes/6.824/FaultTolerance":{"title":"Fault Tolerance","content":"\nSee Also: [Scalability](Scalability.md), [Infrastructure for Applications](Infrastructure%20for%20Applications.md), and [Introduction to Distributed Systems](DistributedSystemsIntro.md)\n\nA single computer can often stay up for years. However, if you're building a system with thousands of computers, that means you're going to have about 3 computer failiures per day. Big problems with big distributed systems turn rare failiure problems to failiure problems that happen all the time. There's almost always something broken: a computer crashed, a piece of the network is down, someone stepped over a cable, a fan is overheating - something always goes wrong. \n\n# Designing around Fault Tolerance\n\nThis means that when you're designing a distributed system, you have to bake in the masking or invisibility of these constant failiures. This makes the programmer's life easier. \n\nSome systems are designed to continue providing, despite partial failiures, undamaged service. These systems are called **available**. The way available systems are specified is that they will stop working after a certain amount of partial failiure.\n\nOther systems will stop working when there are failiures and wait for the failed components to be repaired. But when it does recover, when those components are fixed, the system will continue providing service as if nothing ever went wrong. Systems like these need to do things like save their data on disc. Note available systems can also be recoverable.\n\n## Storage\n\nNon-volatile storage is a good way to keep track of the state of the system. These tend to be expensive to update. \n\nAnother tool is replication. The problem is that the two replicas tend to drift out of sync and stop becoming replicas. \n","lastmodified":"2023-04-04T19:05:28.95757793Z","tags":null},"/notes/6.824/Infrastructure-for-Applications":{"title":"Infrastructure for Applications","content":"\nSee also: [Introduction to Distributed Systems](DistributedSystemsIntro.md)\n\nA lot of our goal is to discover **abstractions**, or ways of simplifying the interface of distributed storage and infrastructure so that it is easy to build applications on top of it. We want to *hide* the fact the system is distributed.\n\n# Implementation \n\n1. Remote Procedure Call (RPC)\n    - Mask the fact you're communicating over an unreliable network.\n\n2. Threads\n    - A way of structuring concurrent operations so that the programmer lives a happier life.\n\n3. Concurrency control\n    - We'll need to spend a certain amount of time thinking about this.\n\n\n\n\n\n\n\n\n\n\n\n\n","lastmodified":"2023-04-04T19:05:28.95757793Z","tags":null},"/notes/6.824/Scalability":{"title":"System Scalability","content":"\nUsually the high-level goal of building a distributed system is to build something with scalable speed-up. It's the idea that with 2 times the computer resources, I should get 2 times the throughput. The alternative is to pay programmers to restructure your software. To get more performance, you can either buy more compute, or better programmers. Compute is cheaper, so more ideal.\n\n## Got a website?\n\nWhen you host a website, it runs on a web server and has a database attached to it. To add more compute so that more users can visit your website, you add more web servers. This kind of scalability is rarely infinite, and as you add more web servers, suddenly the database has too much pressure. ","lastmodified":"2023-04-04T19:05:28.95757793Z","tags":null},"/notes/61c/compvsinterpret":{"title":"Compiler vs Interpreters","content":"\nSee also: [Number Representation](numbers.md)\n\nC works in compilation. It takes a program, and turns it into architecture-specific machine code (a string of 1's and 0's). Architecture-specific because companies like Apple and Intel have their own machine code.\n\nThis is unlike Java, which converts to architecture-independent bytecode that may then be compield by a just-in-time compiler. Java has both a compiler and interpreter.\n\nPython does a realtime bytecode compilation. It compiles to bytecode as it runs.\n\nThese differ mainly in exactly when your prrogram is converted to machine code.\n\n## Compilation\n\n### Advantages\n\nCompiled languages in general are as fast as it gets. It basically runs on raw silicon.\n\nBut then why do we do scientific computation in Python?\n\n- Good libraries for accessing GPU-specific resources.\n- Also, many times python allows the ability to drive many other machines very easily.\n- Also, Python can call low-level C code: Cython\n\n### Disadvantages\n\nThe \"Change -\u003e Compile -\u003e Run\" cycle is slow.\n\nAlso, we have to compile for each architecture we want to run on. This movement is called \"portability\", and is always a pain.\n\n- If you type `make -j` you can compile in parallel, which is quite fast. Linking, however, is still slow (the bottleneck) because of Amdahl's Law.\n","lastmodified":"2023-04-04T19:05:28.95757793Z","tags":null},"/notes/61c/numbers":{"title":"Number Representation","content":"\nThe real world is composed of analog data. As computer scientists, we want to turn that into digital data so we can do things with it.\n\nTo convert, we have to sample and quantize. An example of sampling is asking every 44,100th of a second how loud a music signal is on a CD. An example of quantization is asking how loud the music is in 256 different levels.\n\n# Big Idea: Bits can represent anything!\n\n- Characters\n- ASCII and Unicode\n- Logical Values\n- Colors\n- Locations\n- Addresses\n- Commands\n- Emotions\n\nTypically when we say something is $N$ bits, we mean it's $2^N$ different values.\n\n# Base 10\n\nWe use base 10 because we have 10 fingers. In base 10, we have 10 digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. We can use these digits to represent any number. For example, 1234 as $1 \\times 10^3 + 2 \\times 10^2 + 3 \\times 10^1 + 4 \\times 10^0$.\n\n## Base 2\n\nIn base 2, we have 2 digits: 0 and 1. We can use these digits to represent any number. We denote we're using binary by putting a `0b` in front of the number. So `0b1010` is 10.\n\nFor example, 1010 as $1 \\times 2^3 + 0 \\times 2^2 + 1 \\times 2^1 + 0 \\times 2^0$.\n\n## Base 16\n\nThis is called Hexadecimal. We have 16 digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F. We can use these digits to represent any number.\n\nWe denote we're using hexadecimal by putting a `0x` in front of the number. So `0x10` is 16.\n\n`0xA5` is $10 \\times 16^1 + 5 \\times 16^0 = 160 + 5 = 165$. Note that A, B, C, D, E, F are 10, 11, 12, 13, 14, 15 in base 10.\n\n# Convert from Binary to Hex\n\nAlways left pad with 0s to make it a multiple of 4. Then convert 4 bits at a time using the table below.\n\n| Binary | Hex |\n| ------ | --- |\n| 0000   | 0   |\n| 0001   | 1   |\n| 0010   | 2   |\n| 0011   | 3   |\n| 0100   | 4   |\n| 0101   | 5   |\n| 0110   | 6   |\n| 0111   | 7   |\n| 1000   | 8   |\n| 1001   | 9   |\n| 1010   | A   |\n| 1011   | B   |\n| 1100   | C   |\n| 1101   | D   |\n| 1110   | E   |\n| 1111   | F   |\n\nFor example, `0b1010` is `0xA`. `0b11110` needs to be left padded with 0s to make it a multiple of 4. So it's now `0b00011110`. Then we convert 4 bits at a time. So `0b0001` is `0x1` and `0b1110` is `0xE`. So `0b11110` is `0x1E`.\n\n## Hex to Binary\n\nFrom Hex to Binary is the same process, but in reverse. Just drop leading 0s.\n\n# Decimal vs Hex vs Binary\n\n4 bits is called a nibble. 8 bits is called a byte.\n\n1 hex digit is 16 things. 2 hex digits is 256 things.\n\nColor is usually 0-255 red, 0-255 green, 0-255 blue. This means it's 6 hex digits.\n\n# Which base do we use?\n\nDecimal: Humans\nHex: If human is looking at long strips of binary numbers, it's easier to read hex.\nBinary: Computers will use binary.\n","lastmodified":"2023-04-04T19:05:28.95757793Z","tags":null},"/notes/data-structures/ADT":{"title":"Abstract Data Types","content":"\nAbstract Data types are defined by their operations, not their implementation.\n\n# The Stack ADT\nThe Stack ADT supports the following operations:\n- `push(x)`: Add an element to the top of the stack.\n- int pop(): Remove the top element from the stack and return it.\n\nA linked list and array implementation are basically equally efficient.\n\n# GrabBag ADT\n- `add(x)`: Add an element to the grab bag.\n- int `remove()`: Remove a random element from the grab bag and return it.\n- int `sample()`: Return a random element from the grab bag.\n- int `size()`: Return the number of elements in the grab bag.\n\nArrays turn out to be a better implementation than linked lists.\n\n    \n","lastmodified":"2023-04-04T19:05:28.95757793Z","tags":null},"/notes/data-structures/BFS-Graph":{"title":"BFS and Graph Algorithm Implementations","content":"\n\nSo far, we have seen a few graph traversals;\n- DFS Preorder: returns in order of dfs calls\n- DFS Postorder: returns in order of dfs returns\n- BFS order: returns in order of distance from source\n\nBFS order is analogous to level order traversal.\n\n## Typical BFS Implementation\n\nThe typical BFS implementation is non-recursive. \n- Initialize the fringe (A queue with a starting vertex $s$) and mark that vertex.\n- Repeat until fringe is empty:\n    - Mark vertex $v$ from fringe \n    - For each unmarked neighor $n$ of $v$: mark $n$, add $n$ to fringe, set `edgeTo[n]` to `v`, set `distTo[n] = distTo[v] + 1`.\n\nThe queue data structure is going to enforce the idea that we need to traverse in the order of distance from the source. \n\nAn invariant of BFS is that distance to all items on queue is always $k$ or $k+1$ for some $k$. \n\n## Graph API 1: Integer Vertices\n\nCommon convention: Number nodes irrespective of \"label\", and use number throughout the graph implementation. `Map\u003cLabel, Integer\u003e`.\n\n## Graph API 2: Princeton Graph API\n\n```\npublic class Graph {\n    public Graph(int V);\n    public void addEdge(int v, int w);\n    Iterable\u003cInteger\u003e adj(int v);\n    int V();\n    int E();\n...\n```\n\n- Number of vertices must be specified in advance.\n- Does not support weights on nodes\n- Has no method or getting the number of edges for a vertex (its degree);\n \n\n## Graph Representations\n\n### Adjacency Matrix\n\n\u003c!-- Draw an adjacency matrix for the graph in the previous slide. --\u003e\n\n|   | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| 0 |   |   |   |   |   |   |   |   |   |   |\n| 1 |   |   |   |   |   |   |   |   |   |   |\n| 2 |   |   |   |   |   |   |   |   |   |   |\n| 3 |   |   |   |   |   |   |   |   |   |   |\n| 4 |   |   |   |   |   |   |   |   |   |   |\n| 5 |   |   |   |   |   |   |   |   |   |   |\n| 6 |   |   |   |   |   |   |   |   |   |   |\n| 7 |   |   |   |   |   |   |   |   |   |   |\n| 8 |   |   |   |   |   |   |   |   |   |   |\n| 9 |   |   |   |   |   |   |   |   |   |   |\n\nTakes $O(V^2)$ time complexity.\n\n### Adjacency List\n\nMaintain an array of lists indexed by vertex. This is the most popular representation. This is because graphs are often sparse (not many edges in each bucket).\n\n- Most graph algorithms rely heavily on `adj(s)`.\n\n\n \n","lastmodified":"2023-04-04T19:05:28.95757793Z","tags":null},"/notes/data-structures/BSTheight":{"title":"BST Tree Height","content":"\nHeight varies dramatically between \"bushy\" and \"spindly\" trees.\n\n- Bushy trees have a height of roughly $log(n)$.\n- Spindly trees have a height of $n$.\n\nThis means that for a bushy tree, we have to double the number of nodes to increase the height by 1. For a spindly tree, we have to add 1 node to increase the height by 1. \n- Performance of operations on a spindly tree can be just as bad as a linked list.\n\nImportantly, Big-O and Theta notation are different. Big-O is less precise, and can be analogus to an inequality. If I say that a Binary Tree's height is $O(n^{2})$, what I'm saying in english is\n\n\u003e The height of a binary tree grows less than or equal to n^{2}\n\nThis is still true.\n\nWhereas, if I say a binary tree's height is $\\Theta(log(n))$, what I'm saying in english is\n\n\u003e The height of a binary trees grows equal to $log(n)$ in the best case ($n$ in the worst case).\n\nNote that Big-O is still useful. In the real world, Big-O is shorthand for \"in the worst case\". \n- It allows us to make simple blanket statements\n- Sometimes we don't know the exact runtime, so we use Big-O to give an upperbound\n- It can be easier to write proofs for Big-O than Big-Theta.\n\n\n\n\n","lastmodified":"2023-04-04T19:05:28.95757793Z","tags":null},"/notes/data-structures/BStperformance":{"title":"BST Performance","content":"\nHeight and average depth are important properties of BSTs:\n- The depth of a node is how far it is from the root\n- The height of a tree is the depth of its deepest leaf\n- The average depth of a tree is the average depth of a tree's nodes.\n\n$$ \\text{Average Depth} = \\frac{\\text{Total Depth}}{\\text{Number of Nodes}} $$  \n\nAdding in linear order will result in a spindly tree. Adding in random order will result in a bushier tree.\n\nWe now know and have seen in great detail is that \n- the worst case is $\\Theta(n)$\n- the best case is $\\Theta(log(n))$\n\nRandom trees have an average depth of $\\Theta(log(n))$. In other words, random trees are bushy, not spindly.\n\n# Mathemetical Analysis\n\nThe average depth of a tree is $ ~2log(n) = \\Theta(log(n))$. The tilda (~) is can kind of be thought of as $\\Theta$ but we don't drop the constant. Thus the average runtime for `contains` is $\\Theta(log(n))$ on a tree built with random inserts.\n\nIf $N$ distinct keys are inserted into a BST in random order, the expected height of the tree is $~4.311log(n)$. This is beyond the scope of this text (the proof was made in 2003).\n\n# Bad news\n\nWe can't always insert our items in random order. This is because Data comes in overtime! We don't have it all in advance. If you're storing a bunch of dates, your binary tree will be horrible. \n\n\n\n\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/Btree":{"title":"B-Tree","content":"\nB-Trees are a kind of self-balancing BST. \n\n# B-Trees\n\nTo start, let's make a rule to never add a new leaf. Instead, we'll just add any new value to a node.\n\nBut this node can get quite big, and our data structure will degenerate to a linked list. That's no good. Instead, let's set a limit $L$ on the number of items\nthat can be stored in our node. \n\n- If a node has more than L items, give an item to parent.\n- Point parent to new node that was split off.\n\nExamining a node costs us $O(L)$ compares, but that's fine since L is constant.\n\n# What if we split non-leaf nodes?\n\n- Observation: Splitting-trees have perfect balance. \n  - If we split the root, every node gets pushed down a level.\n  - If we split an internal node, the height doesn't change. \n\n# Terminology\n\nSplitting Trees are B-Trees. B-Trees of order $L = 3$ are called 2-3-4 trees or 2-4 trees. \n- B-Trees trees of order $L = 2$ are called 2-3 trees. \n- The \"2-3-4\" refers to the number of children a node can have. For example, a 2-3-4 tree has 2, 3, or 4 children.\n\n\nNote that the origin of the name \"B-Tree\" is unclear. \"Broad\", \"Bushy\", and \"Balanced\" are all possible explanations. The most likely one is \"Bayer\", since the first paper on B-Trees was written by Bayer.\n\n# B-Trees in Practice\n\n- B-Trees are used in databases and file systems. L would be very large (in the thousands).\n- Conceptually simple balanced search tree.\n\n\n\n\n\n\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/bstoperations":{"title":"BST Operations","content":"\n# Search\n\nIf searchKey = T.key, return.\n- If searchKey \u003c T.key, search T.left;\n- If searchKey \u003e T.key, search T.right;\n\nRuntime to complete search on a bushy BST in the worst case is $O(log(n))$. This is intuitive to see and you can generally estimate the number of computations needed to search a BST by the height of the tree.\n\nRemember, the height of the tree is roughly $log(n)$. So if the tree is 15 nodes, $log(15) = 4$. So the worst case runtime is $O(4) = O(1)$.\n\nBushy BSTs are extremely fast.\n\nAt 1 microsecond per operation, can find something from a tree with $10^{300000}$ nodes in one second. \n\nMuch (perhaps most?) computation is dedicated towards finding things in response to queries. \n\n# Insert \n\nSearch for key. \n- If found, do nothing.\n- If not found \n  - Create a new node\n  - Set appropiate link. \n\n# Delete\n\n3 cases: \n\n- Deletion key has no children\n- Deletion key has one child\n- Deletion key has two children\n\n\nIn the first case, we just sever the link. It gets collected by the garbage collector.\nIn the second case, we replace the node with its child. Note that even though the deletion key might still be pointing to the child, it will still get collected by the garbage collector.\nIn the third case, we replace the node with its predecessor or the successor. Note that the predecessor or successor are guarenteed to have at most one child.\n\nThis is usually known as Hibbard deletion.\n\n# BSTs as Sets and Maps\n\n\nThe BST we created could be thought of as a set. But we can also use it as a map. This is done by storing each BST node as key/value pairs. Note that there is no efficient way to look up a value in a BST.\n- Example: Cannot find all the keys with value = 1 without iterating over ALL nodes. This is fine. \n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/btreeinvariant":{"title":"B-Tree Analysis","content":"\n# Bushiness and Invariants\nUnlike in BSTs, where if you insert things in order the tree will be spindly, in B-Trees, if you insert things in order, the tree will be bushy no matter what.\n\nWe get two nice invariants:\n- All leaves are the same distance from the source\n- A non-leaf node with $k$ items must have $k+1$ children\n\nThese invariants guarentee a bushy tree.\n\n# Runtime Analysis\n\nLet's analyze the runtime of a `contains` operation.\n- In the worst case, we're going to need to check $H+1$ nodes, where $H$ is the height of the tree.\n- In the worst case, we're going to need to check $L$ items in each node.\n\nThis means our runtime will be $O((H+1)L)$ which is $O(HL)$.\n\nHowever, we know that $H = \\Theta(log(n))$, so $O(HL) = O(Llog(n))$ = $O(log(n))$. Note L is a constant, so we can drop it.\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/graph":{"title":"Graphs","content":"\nTrees are fantastic for representing strict hierarchial relationships.\n- But not every relationship is hierarchial.\n\nA graph consists of:\n- a set of nodes\n- a set of zero or mor edges, which connects two nodes.\n\n# Simple Graphs\n\nA simple graph is a graph with:\n- no edges that connects a vertex to itself, i.e. no loops\n- No two edges that connect the same vertices, i.e. no parallel edges.\n  \nNote you can think think of edges as a pair of vertices.\n\nVertices with an edge between are called adjacent.\n\nVertices or edges may have **labels** or **weights**.\n\nA **path** is a sequence of vertices such that each pair of consecutive vertices is connected by an edge.\n\nA cycle is a path that starts and ends at the same vertex.\n  \nA graph with a cycle is called **cyclic**. A graph without a cycle is called **acyclic**.\n\nTwo vertices are **connected** if there is a path between them. If all vertices are connected, the graph is connected.\n\nThere is other terminology, like vertex degree and connected components.\n\n\n  ","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/graph-problems":{"title":"Graph Problems","content":"\n# Graph Queries\n\nThere are a lot of interesting questions we can ask about a graph. What is the shortest path between two points? Are there cycles? What is the longest path without cycles?\n\nIs there a path we can take that only uses each node exactly once? Is there a tour that uses each edge exactly once?\n\nWhat's interesting about these problems is that they're solved with traversals.\n\n# Well known Graph Problems\n\n- **s-t path problem**: Given a graph and two vertices s and t, is there a path from s to t? (GPS)\n- **Connectivity**: Is the graph connected? I.e. is there a path between all vertices? (A generalization of the s-t path problem)\n- **Bi-connectivity**: Is there a vertex whose removal would disconnect the graph? (Biconnectivity might be a bad because you'd have a single point of failure)\n- **Shortest s-t path**: Given a graph and two vertices s and t, what is the shortest path from s to t? (Google Maps)\n- **Cycle detection**: Is there a cycle in the graph?\n- **Euler Tour**: Is there a cycle that uses each edge exactly once?\n- **Hamiltonian Tour**: Is there a cycle that uses each vertex exactly once?\n- **Planarity**: Can you draw the graph on paper with no crossing edges?\n- **Isomorphism**: Are two graphs the same? \n\nWe often can't tell how difficult a graph problem is without very deep consideration. \n\nAn efficient Euler tour algorithm O(# edges) was found as early as 1873. But despite decades of intense study, no efficient algorithm for a Hamilton tour exists. Best algorithms are exponential time.\n\nGraph problems are among the most mathemtically rich areas of CS theory.\n\n\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/hash/hash-collisions":{"title":"Hashing Collisions","content":"\nPigeonhole principle tells us that collisions are inevitable due to integer overflow.\n- hash code for \"moo\" and \"nep\" might both be 718.\n\nThe way we deal with this is creating \"buckets\" where a collision may occur. This can look like anything; linked lists, arraylists, etc. \n\nThat means the runtime for `contains` and `insert` will be $O(Q)$ where $Q$ is the length of the longest list. \n\nOne more innovation: We don't need billions of buckets or indexs. We can reduce them. For example, if our hash code is 123109, we can apply `% 10` to get 9.\n\nThis is basically what a hash table is.\n\n# The pipeline\n\nWe start with something we want to store, say the word 'hi', inside the hash table. We then compute the hash code using a hash function. We reduce the hash code in some way, and store it at that index. \n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/hash/hash-intro":{"title":"Hashing Intro","content":"\nOur search trees have excellent $O(log(n))$ performance. Can we do better? And can we do it without comparisons?\n\n# Approach: Taking advantage of arrays\n\nThe idea is simple: Create a boolean array of $N$ length that is initialized as false. Whenever we add an element, we set the corresponding index to true. \n\nThis makes `contains` and `add` O(1).\n\nThe draw back is that we use a ridiculous amount of memory, and we don't yet have a way to generalize data types. \n\n# ASCII and Unicode\n\nThe ASCII Standard is the most basic character set used by computers. It is a 7-bit character set, which means it can represent 128 different characters.\n\nIf we want to represent more characters, we can use Unicode. Unicode is a 16-bit character set, which means it can represent 65,536 different characters. This would include things like Chinese characters, emojis, and other symbols.\n\nWe can use the ASCII and Unicode character sets to create a hash function.\n\n# Integer Overflow\n\nIn Java, the largest possible Integer is 2,147,483,647. If we add 1 to this number, we get -2,147,483,648. This is called integer overflow, meaning we start back at the smallest possible integer.\n\nThis means that if we represent words in ASCII, which is base 128, we can only represent 2,147,483,647 words. Which means we can get collisions. \n\nTHe term we use is 'hash code' to refer to the integer that we get from our hash function. Wolfram Alpha says that a hash code \"projects a value from a set with many (or even infinite) members to a value from a set with a fixed (fewer) number of members.\"\n\nThe pigeonhole principle tells that we will get collisions. It is inevitable.\n\nThat leaves us with two fundamental challenges:\n\n1. How do we avoid collisions?\n2. How do we compute the hash code for arbitrary objects?\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/hash/hash-performance":{"title":"Hash Performance","content":"\nUnfortunately, the hash table we devised is $O(Q)$ for its operations, and $Q$ has $O(N)$ order of growth.      \n\nThis is really bad, because eventually our operations will get slow.\n\nWe can fix this by adding more buckets!\n\n# Model\n\nSuppose we have:\n- M buckets\n- An increasing number of $N$ items.\n\nAn example strategy: When $\\frac{N}{M}$ is $\\geq$ 1.5, then double M.\n- $\\frac{N}{M}$ is known as the `load factor`. It represents how full the hash table is. \n\nAs long as $M = \\Theta(N)$, then $N = \\Theta(\\frac{N}{M}) = O(1)$.\n\nOne thing to note is that when we do our resizing, items in our table will shuffle around and the lists will get shorter. \n- Resizing takes $O(N)$ time.\n- Most `add` operations will take $O(1)$ time. Some will take $O(N)$ time to resize.\n-  Also, the specific load factor constant doesn't matter. We will still get $O(1)$ time.\n\nMore specifically, with resizing, `contains` ia $\\Theta(1)^{†}$ and `add` is $\\Theta(1)^{* †}$. `†` means that we assume items are evenly spread, and `*` means \"on average\".\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/heap":{"title":"Heaps","content":"\nHeaps are a pretty famous data structure. Note that they're not at all related to 'the heap', something you'd learn in an operating systems class.\n\n# Introducing the Heap\n\nIt is a binary tree with the following properties:\n- It is a complete binary tree (all levels are filled except the last, which is filled from left to right)\n- It is a heap-ordered binary tree (the key in each node is larger than the keys in its children)\n\n# Heap Operations\n\n## Insert\n\n- Add the new node to the bottom level of the heap, as far left as possible\n- Swim the new node up until it is larger than its parent, or it is the root\n\n## Delete Smallest\n\n- Swap the root with the node at the bottom level, farthest right\n- Sink the new root down until it is smaller than both of its children, or it is a leaf\n\n## Get Smallest\n- Return the root","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/pq":{"title":"Priority Queue","content":"\nA priority queue allows tracking and removal of the smallest item.\n\nthe interface;\n- `add`\n- `getSmallest`\n- `removeSmallest`\n\nIt can transform a problem which requires a lot of memory into one which requires much less. \n\n# Implementation\n\nWe could use an ordered array, but adding and removing elements would be $O(N)$. We could also use a bushy BST, and for add, removr, and getSmallest, we would have $O(\\log N)$ time. The issue is the possibility of duplicates, which is very awkward to implement/fix (though possible!). We could also use a hash map, and adding would be $O(1)$, but removing and getting the smallest would be $O(N)$ because we don't know what bucket the smallest item is in.\n\nSo what we need is a heap. It will give us $O(\\log N)$ for all operations, and it will also allow duplicates.\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/red-black-tree":{"title":"Red Black Trees Intro","content":"\n\u003e \"Beautiful algorithms are, unfortunately, not always the most useful.\" - Donald Knuth\n\nB-Trees are painful to implement. And even if you do implement them, they're still not that great. They're $O(log(n))$, but there are better $O(log(n))$ data structures out there.\n\nSome of the things that make it painful:\n\n- Maintaining different node types.\n- Interconversion of nodes between 2-nodes and 3-nodes\n- Walking up the tree to split nodes.\n\n# The idea\n\nIn a normal BST, we can get different BSTs depending on our insertion order. \n- In general, for $N$ items, there are [$\\textnormal{Catalan}(N)$](https://en.wikipedia.org/wiki/Catalan_number) different BSTs.\n\nBut given any BST, we can move to a different configuration using a rotation. \n\nRotation definition: A rotation is a transformation of a tree that preserves the BST property.\nExample: A left rotation on a node $x$ is a transformation that moves $x$'s right child to $x$'s position, and moves $x$ to $x$'s right child's position. \n\nWe can shorten (or lengthen) a BST by performing a rotation. There is a paper proving we can do an optimal set of rotations in $O(n)$. \n \n\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/red-black-tree-def":{"title":"Red Black Trees Definition","content":"\n\n Red Black Trees are trees that are structurally indentical to B-Trees, but have glue links. This idea is commonly used in practice  (ex: `java.util.TreeSet`).\n\n A BST with left glue links are called a **left-leaning red-black BST**. \n - LLRBs are normal BSTs.\n - There is a 1-1 correspondence between an LLRB and an equivalent 2-3 tree. \n - The red is just convenient fiction. Red links don't \"do\" anyything special.\n\n# Red Black Tree Properties\n\nFrom this, we get two very important properties:\n- No node has two red links connected to it.\n- Every path from a leaf node to the root has the same number of black links.\n\nTherefore, LLRBs are **perfectly balanced**.\n\nThe way we'd implement this is to insert as usual into a BST, and then do some number of rotations to maintain the 1-1 mapping. \n\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/redblacksummary":{"title":"Red Black Trees Performance","content":"\n# LLRB Runtime\n\n- LLRB has height $O(log(n))$\n- Contains is trivially $O(log(n))$\n- Insert is $O(log(n))$\n  - $O(log(n))$ to add new node\n  - $O(log(n))$ to fix up tree (rotation and color flips)\n\nWe won't discuss delete because it's uninteresting.\n\n#  Summary\n\n- BSTs are simple, but they are subject to imbalance.\n- 2-3 Trees (B-Trees) are balanced, but are painful to implement and are slow.\n- LLRBs insertion is simple to implement (but delete is hard).\n  - Works by maintaining a mathematical bijection with 2-3 trees.\n- Java's TreeMap is a red-black tree (not left leaning).\n - Maintains correspondence with a 2-3-4 tree (is not a 1-1 correspondence).\n - Allows glue links on either side\n - More complex implementation, but significantly (?) faster.\n\n# Footnote\n\nThere are many other types of search trees out there. AVL trees, splay trees, treaps, etc. \n- And there are other efficient ways to implement sets and maps entirely.\n    - Skip lists\n    - Hashing (the most common alternatives)\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/rrbinsert":{"title":"Red Black Trees Rules","content":"\n# Red Black Tree Rules\n\n\nWhen inserting: use a red link\n- If there is a right leaning \"3-node\", we have a left leaning violation.\n - Rotate left \n- If there are two consecutive left links, we have an incorrect 4-node violation\n    - Rotate right\n- If a node has two red link children, we have a temporary 4-node violation\n    - Flip colors\n\n\nOne last detail: Cascading operations\n- It is possible that a rotation or flip will cause an additional violation that needs fixing.\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/s-t":{"title":"S-T Connectivity","content":"\nSee Also: [Graph Problems](graph-problems.md)\n\nLet's solve a classic graph problem called s-t connectivity problem.\n- Given source vertex $s$ and a target vertex $t$, is there a path from $s$ to $t$?\n\n# Recursive Solution\n\nOne possible recursive solution goes as follows:\n- Mark $s$\n- Does `s == t`? If so, return true.\n- Else, `if connected(v,t)` for any unmarked neighbor $v$ of $s$, return true.\n- Return false.\n\nEach vertex is visited at most once.\n- Marking nodes prevents multiple visits (infinite loops).\n\n# Depth First Traversal\n\nThe idea of exploring a neighbor's entire subgraph before moving on to the next neighbor is known as Depth First Traversal.\n\n## Depth First Paths \n\nFind a path from $s$ to every reachable vertex, visiting each vertex at most once. `dfs(v)` is as follows:\n\n- mark `v`\n- For each unmarked adjacent vertex `w`\n  - set `edgeTo[w]` to `v`\n  - `dfs(w)`\n\n\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/shortest-paths":{"title":"Shortest Paths","content":"\nSee also: [Graph Problems](graph-problems.md), [S-T Connectivity](s-t.md), [Tree vs Graph Traversals](tree-vs-graph.md)\n\n# Graph Problems Summary\n\n\u003c!-- Create a table with: problem, problem description, solution, eficiency --\u003e\n| Problem | Description | Efficiency |\n| --- | --- | --- |\n| S-T paths | Find a path from `s` to every reachable vertex |  | $O(V+E)$ |\n| S-T shortest paths | Find a shortest path from `s` to every reachable vertex |  | $O(V+E)$ |\n\nLast time, we saw two ways to find paths in a graph: DFS and BFS.\n\nWhich is better?\n\n    ","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/sorts":{"title":"Sorting","content":"\n# Sorting \n\nInformally: Given items, put them in order. This is useful because can do things like:\n- Allow rapid duplicate finding\n- -Enable binary search\n- Convert into various data structures\n\n## Knuth's Definition\n\nAn ordering relation `\u003c` for keys `a`, `b`, and `c` is has the following properties:\n- Law of trichotomy: `a \u003c b` or `b \u003c a` or `a = b` is true\n- Law of transitivity: if `a \u003c b` and `b \u003c c`, then `a \u003c c` is true\n\nAn ordering relation with the properties above is also known as a \"total order\".\n\nA **sort** is a permutation of a sequence of elements that puts the keys into non-decreasing order relative to a given ordering relation.\n- $x_1 \\leq x_2 \\leq x_3 \\leq \\dots \\leq x_n$\n\n\n## Inversions\n\nWe can have an alternate perspective of sorting. An **inversion** is a pair of elements that are out of order with respect to `\u003c`. \n\nFor example, in the sequence $[1, 3, 2, 4]$, the pair $(3, 2)$ is an inversion.\n\nAnother way to think about sorting:\n- Give na sequence of elements with $Z$ inversions\n- Performa a sequence of operations that reduces inversions to $0$.\n\n\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/tree-traversal":{"title":"Tree Traversals","content":"\nUnlike a linked list, where you can traverse the list by following the `next` pointers, a tree is a bit more complicated. There are more ways to traverse a tree.\n\n## Level Order Traversal\n- Visit top-down, left to right (like reading in English)\n\n## Depth First Traversals\n\nThere are three types of depth first traversals:\n- Preorder\n- Inorder\n- Postorder\n\nThe basic idea is to traverse \"deep nodes\" before shallow ones. Note that traversing a node is different than \"visiting\" a node. \n\n## Preorder\n\nA PreOrder traversal is a depth first traversal where the root is visited first, then the left subtree, then the right subtree. \n  \n``` \npreOrder(BSTNode x){\n    if(x == null) return;\n    print(x.key);\n    preOrder(x.left);\n    preOrder(x.right);\n}\n```\n\n## Inorder\n\nA InOrder traversal is a depth first traversal where the left subtree is visited first, then the root, then the right subtree. \n\n```\ninOrder(BSTNode x){\n    if(x == null) return;\n    inOrder(x.left);\n    print(x.key);\n    inOrder(x.right);\n}\n```\n\n## Postorder\n\nA PostOrder traversal is a depth first traversal where the left subtree is visited first, then the right subtree, then the root. \n\n```\npostOrder(BSTNode x){\n    if(x == null) return;\n    postOrder(x.left);\n    postOrder(x.right);\n    print(x.key);\n}\n```\n\n# Handy Trick\n\nTo visualize a traversal, you can do the following:\n\nTrace a path around the graph, from top going counter-clickwise.\n- Preorder: Visit node everytime we pass the left of a node\n- Inorder: Visit node everytime we pass the bottom of a node\n- Postorder: Visit node everytime we pass the right of a node","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/tree-vs-graph":{"title":"Tree vs Graph Traversals","content":"\nJust as there are many tree traversals:\n- Preorder\n- Inorder\n- Postorder\n- Level Order\n\n## DFS Preorder\n\nWhat we did in DepthFirstPaths is called \"DFS Preorder\": Action is before DFS.\n\nIn a graph, we choose a particular source. We also have to decide how to deal with \"tie-breaking\".\n\n## DFS Postorder\n\nWe could also do a \"DFS Postorder\" traversal, where our action is after the DFS.\n\nexample:\n\n`dfs(s)`: \n- mark s\n- for each unmarked neighbor `n` of `s`, `dfs(n)`\n- print s\n\nThis would print the nodes in order of dfs returns. \n\nThese are analogs to the tree traversals. There is also an analog to level order traversal.\n\n# BFS order\nBFS stands for Breadth First Search. \n\nBFS Order: Act in order of distance from source.\n- Analogous to level order traversal\n- Search is wide, not deep\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/data-structures/trees":{"title":"Trees","content":"\n\nA tree consists of \n- A set of nodes\n- A set of edges that connect those nodes\n  - Constraint: There is exactly one path between any two nodes.\n\nIn a rooted tree, we call one node priviledged node the root.\n- Every node N except the root has exactly one parent, deifned as the first node on the path from N to the root.\n- The root is usually depicted as tthe top of the tree (kind of backwards from how we usually think about trees)\n- A node with no child is called a leaf.\n\nIn a rooted binary tree, every node has either 0, 1, or 2 children (subtrees).\n\nA binary search tree is a rooted binary tree with one additional property: the BST property.\n\nBST property: For every node X in the tree: \n- Every key in the left subtree is less than X's key\n- Every key in the right subtree is greater than X's key\n\nNote that there is a difference between a Binary Tree, and a binary search tree. A binary tree is a tree where every node has at most two children. A binary search tree is a binary tree where the BST property holds.\n\nOrdering must be complete, transitive, and antisymmetric. Given keys p and q:\n- Exactly one p \u003c q or q \u003c p are true.\n- if p \u003c q and q \u003e r, then that implies p \u003c r  \n\nOne consequence of these rules : No duplicate keys are allowed in a BST.\n- keeps things simple. Most real world follow this rule.\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/ds/introdb":{"title":"Introduction to Databases","content":"\nThe simplest database we can have is a bunch of CSV files that we can manage with our own code. For every entity, we'll store them in our own file (album.csv, artists.csv). The application will parse / read / write.\n\nThere are a lot of issues with this approach:\n\n- Data Integrity\n  - invalid strings, typos, etc\n- Implementation\n  - Slow\n  - What if we want to create a new application with the same database?\n  - What if two threads try to write to the same file at the same time.\n- Durability\n  - What if the machine crashes while our program is updating a record?\n  - What if we want to replicate the database on multiple machines for high availability?\n\nFor these problems and others, we want to offload that complex logic. We use a **Database Management System** (DBMS). We can define, create, query, update, administer databases (we assume DB's are on disk).\n\nEarly DBMs were hard to build and maintain. Tight coupling between logical and physical layers. You have to know what queries your app would execute before you deployed the database.\n\nEdgar Codd proposed the relational model. The first paper came out on 1969 and the one that most people read was in 1970. He proposed:\n\n- Store database in simple data structures (the idea of a relation isn't like being related to your parents - it's synonymous with tables)\n- Access data data through high-level languages (back then, we were writing explicit produral code to interact with the database)\n- Physical storage left up to implementation (How things are stored aren't a part of the relational model - this allowed flexibility when it came to change how things are stored, which would often depend on how an application might scale overtime)\n\nThere are other data models, which are ways we can describe the data in a database.\n\nAt the top we have the relational model. Than we have NoSQL, which are key/value, document/json, graph models, column/family. Array / Matrix databases are used in machine learning, we mostly use dataframes. There are hierachical and Network datamodels but are esoteric, obsolete/rare, and are seen in legacy applications. They are remnants of old software, don't use them.\n\n## Relational Model\n\nThere are three components of the relational model:\n\n- Structure: The definition of relations and their contents\n- Integirty: Ensure the database's contents satisfy constraints.\n- Manipulation: How to access and modify a database's contents.\n\nA relation is an unordered set that contains the relationship of attributes that represent entities (ARtist(name, year, country).\n\nA **tuple** is a set of attribute values (also known as its domain) in the relation.\n\n- Values are (normally) atomic/scalar (this particular constraint was placed by Codd in the 70s, but is now relaxed. We can have array/json objects as values in modern DBMSs).\n- The special value `NULL` is a member of every domain.\n\nAn `n-ary` relation means a Table with $n$ columns.\n\nA relation's **primary key** uniquely identifies a single tuple. Some DBMSs automatically create an internal primary key if you don't define one. Auto-generation of a unique integer primary keys:\n\n- Sequence(SQL: 2003)\n- Auto_Increment (MySQL)\n\nA foreign key specifies that an attribute from one relation has to map to a tuple in another relation.\n\n## Data Manipulation Languages (DML)\n\nHow do we store and retrieve information from a database?\n\n- Procedural (Relational Algebra)\n\n  - The query specifies the (high-level) strategy the DBMS should use to find the desired result.\n\n- Non-Procedural (Relational Calculus)\n  - The query specifies only what data is wanted and not how to find it.\n\n## Relational Algebra\n\nEdgar Codd proposed 7 fundamental operators in relational algebra to retrieve and manipulate tuples in a relation.\n\n- Select, Projection, Union, Intersection, Difference, Product, Join\n\nEach operator takes one or more relations as its inputs and outputs a new relation.\n\n- We can \"chain\" operators together to create more complex operations.\n\n### Select\n\nThe `SELECT` operator chooses a subset of the tuples from a relation that satisfies a selection predictate\n\n- The predictate acts as a filter to retain only tuples that fulfill its qualifying requirement\n- Can combine multiple predicates using conjunctions / disjunctions.\n\nThe syntax: $ \\sigma_{\\textnormal{predictate}}(R). $\n\nIn SQL this would be:\n\n```sql\nSELECT * FROM R WHERE a_id = 'a2' OR b_id = '103';\n```\n\n### Projection\n\nGenerate a relation with tuples that contains only the specified attributes.\n\n- Can rearrange attributes ordering\n- Can manipulate the values\n\nSyntax: $ \\Pi_{\\textnormal{attributes}}(R) $\n\nIn SQL this would be:\n\n```sql\nSELECT b_id-100, aid FROM R WHERE a_id = 'a2';\n```\n\n### Union\n\nGenerate a relation that contains all tuples that appear in either only one or both input relations.\n\nSyntax: $ R \\cup S $\n\nIn SQL this would be:\n\n```sql\nSELECT * FROM R\nUNION ALL\nSELECT * FROM S;\n```\n\n### Intersection\n\nGenerate a relation that contains all tuples that appear in both input relations.\n\nSyntax: $ R \\cap S $\n\nIn SQL this would be:\n\n```sql\nSELECT * FROM R\nINTERSECT\nSELECT * FROM S;\n```\n\n### Difference\n\nGenerate a relation that contains all tuples that appear in the first relation but not the second.\n\nSyntax: $ R - S $\n\nIn SQL this would be:\n\n```sql\nSELECT * FROM R\nEXCEPT\nSELECT * FROM S;\n```\n\n### Product\n\nGenerate a relation that contains all possible combinations of tuples from the input relation (cross product, cartesian product).\n\nSyntax: $ R \\times S $\n\nThis might sound stupid but it's useful for something like testing where you want to test every possible combination of inputs / configurations.\n\n### Join\n\nGenerate a relation that contains all tuples that are a combination of two tuples (one from each input relation) with a common value for one or more atttributes.\n\nSyntax: $ R \\bowtie S $\n\nIn SQL this would be:\n\n```sql\n SELECT * FROM R NATURAL JOIN S;\n```\n\n## More operators\n\nThere are additional operators that people have added to the relational algebra.\n\n- Rename ($ \\rho $)\n- Assignment ($ R \\leftarrow S $)\n- Duplicate Elimination ($ \\delta $)\n- Aggregation ($ \\gamma $)\n- Sorting ($ \\tau $)\n- Division ($ R / S $)\n\n## Observation\n\nRelational algebra still defines the high-level steps of how to compute a query. But this can still be inefficient because even though two queries might yield the same result, the performance of the two queries might be different because of the implementation details.\n\nA better approach is to state the high-level answer that you want the DBMS to compute.\n\n- Retrieve the joined tuples from $ R $ and $ S $ where `b_id = 102`.\n\nThe relational model is indepnende ot any query language implementation.\n\n## Conclusions\n\nDatabases are ubiquitous.\n\nRelational algebra defines the primitives for processing queries on a relational database.\n\nWe will see relational algebra again when we talk about query optimization + execution.\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/life/minimal":{"title":"Minimalistic Workout","content":"\n# Full Body Day 1\n\nWarm up: Brisk Walking and Dynamic Stretches\n\n1. Flat Dumbbell Press: 1x4-6 reps heavy 1x 8-10 reps backoff weight\n2. Dumbbell Romanian Deadlift: 3x8-10\n3. 2-grip Lat Pulldown: 2x10-12 reps\n4. Dumbbell Step Up: 1x8-10\n5. Overheap Cable Triceps Extension 1x12-15 reps + Dropset\n6. Machine Lateral Raise: 1x12-15 reps\n7. Leg press toe raise 1x12-15 reps\n\n\n# Full Body Day 2\n\nWarm up: Brisk Walking and Dynamic Stretches\n\n1. Hack Squat: 1x4-6 reps + 1x8-10 reps\n2. Antagonistic Superset\n    - T-bar row: 2x10-12\n    -  High Incline Smith Press: 2x10-12\n3. Replace with dumbbell\n4. Seated Leg Curl: 1x10-12\n5. EZ-Bar Bicep Curl: 1x12-15\n6. Cable Crunch: 1x12-15\n\n\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/scalability/autoscaler":{"title":"Autoscaling","content":"\n## How many servers do we need\n\nWe can use benchmarking here - extrapolating based off how many users one server can handle. But it might be the case that our user count fluctates (as in the case of a newspaper website), so we can't just use one number.\n\nIn this case, let's consider the maximum. So that no matter how many users we get, we can handle it. But even in this case, it's still not the most economical choice. In the vast majority of cases, we'll have too few users, and we'd still be paying for the servers - electricity, maintenance, etc.\n\n## Auto Scaling (Cloud)\n\nOne solution to this is **autoscaling**. The load balancer will automatically add or remove servers based on the number of users. Often, there is a minimum and maximum number of servess that can be configured and which can be scaled up or scaled down. There are tradeoffs here too, where it will take time for the autoscaler to add or remove servers. If a lot of users come at once, the autoscaler might not be able to keep up, or service them immediately. \n\n\n\n\n\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/scalability/scalability":{"title":"Load Balancing","content":"\n## Scalability\nWhen we deploy a website, we need to put it on a server. Users will send requests to the server, and the server will respond. But if a lot of users send requests at the same time, the server will get overloaded, so we have to think about scalability.\n\nFirst, let's think about where those servers are.\n\n### Servers\n\nServers exist in two places:\n- 1. The Cloud\n- 2. On-Premise - servers inside a company's building\n\nIn an on-premise server, the company owns the server, and they can do whatever they want with it. They can install whatever software they want, and they can control the hardware.\n\nWhereas in the cloud, the company doesn't own the server. They rent it from a cloud provider, and they can't control the hardware. They can only control the software. However, this has its own benefits. \n\nFor example, if a company has a lot of data, they can rent a lot of servers, and they can distribute the data across those servers. This is called horizontal scaling. \n\nThe question \"How many users can my server handle?\" is called the scalability question. It varies at any given time.\n\n\n### Vertical Scaling\n\nVertical scaling is when you increase the power of a single server. For example, you can increase the amount of RAM, or you can increase the number of cores. This approach is simple: swapping out a server is easy. But there are still limits.\n\n### Horizontal Scaling\n\nHorizontal scaling is when you add more servers. This is more complicated, because you have to distribute the data across the servers. But it's more scalable. \n\nBut there has to be some logic for deciding what server a user should go on. When a user sends a request to the server, how does it know which server to send the request to? \n\n### Load Balancer \n\nThis is called a load balancer and the problem is called load-balancing. t's a piece of software that sits in front of the servers, and it decides which server to send the request to. \n\nHow does the load balancer decide? There are many different approaches:\n\n#### Random Choice\n\nWe can just randomly choose a server. This is simple, but it's not very efficient. We might end up with servers that are entirely unused. \n\n#### Round Robin\n\nSend the request to the first server, then the second, then the third, then the first, then the second, then the third, etc.\n\nThis is also relatively simple. But it also suffers from inefficiency. Some requests will take longer than others, so some servers will be overloaded, and some will be underloaded.\n\n#### Fewest Connections\n\nSend the request to the server with the fewest connections. This is a good approach, but it still has its drawbacks. Computing the number of connections might be expensive, and it might not be accurate.\n\nAll of these problems suffer from one more problem: sessions. \n\nA session is a connection between a user and a server. When a user sends a request to the server, the server creates a session, and it sends a cookie to the user. The cookie is a small piece of data that the user sends back to the server with every request. Basically, the cookie is a way for the server to identify the user and store information about the user.\n\nSo the issue is that if a user sends a request to the server, and the server sends the request to a different server, the cookie won't be sent back to the server. So the server won't know who the user is. \n\nHow do we load-balance in a session-aware way?\n\n### Session-Aware Load Balancing\n\n#### Stick Sessions\n\nThe load balancer will remember what server the user was connected to last time, and it will send the user to that server. The tradeoff is that the server might be overloaded. There's a difference in utilization between servers.\n\n#### Sessions in Database\n\nRather than storing the session on the server, we can store it in a database. This way, the load balancer can send the user to the server that has the session.\n\n#### Client-Side Sessions\n\nStore the session with cookies. You can store information about what user is currently logged in. But it can be manipulated - in which case you have some sort of encryption. \n\n\n\n\n\n\n\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/notes/sysdesign/computerarchitecture":{"title":"Computer Architecture","content":"\n## A high level overview of how computer architecture\n\n### Disks and RAM\n\nWe'll start with the disk.\n\nDisks will store all our data (SSD, HDD) persistently. Writing and reads are measured in milliconds $(ms)$.\n\nWe also have **RAM** (random access memory), which is a lot smaller. However, it's quite fast - writing and reading to it is measured in microseconds $(\\mu s)$.\n\nRAM is an order of magnitude faster than disks.\n\n### CPU\n\nHow do we write/read our disk and RAM? What if we wanted the disk and RAM to communicate with each other?\n\nHere we introduce **CPU** (Central Processing Unit). When we talk about read or write speed, we talk about how fast the CPU can write or read. The CPU really only knows how to do computations - arithmetic, logical, bit, etc.\n\n### Cache\n\nWhile we know that the CPU can write to RAM in terms of microseconds, we can go even faster. Introduce the cache, which is measured in megabytes (so, considerably smaller than RAM). Its primary job is to make reads and writes to RAM faster. It takes a subset of the data in RAM and stores it in the cache, so that when we want to read or write to RAM, we can do it 10 or 100 times faster.\n\nThe drawback of cache and RAM is persistence: if we turn off the computer, all the data in the cache and RAM will be lost. So we need to store it somewhere else (the disk).\n\n\nThis is a high level overview of how computers, and when it comes to designing distributed systems, we'll find parallels from how a single computer works to how a distributed system works.\n\nOne of the bottlenecks is CPU - how fast we execute our code. Moore's law says that every 2 years, the number of transistors on a chip will double. However, this is plateauing, and we're seeing diminishing returns.\n\nWe can solve bigger problems by using more computers.\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/private/ds/bstoperations":{"title":"BST Operations","content":"\n# Search\n\nIf searchKey = T.key, return.\n- If searchKey \u003c T.key, search T.left;\n- If searchKey \u003e T.key, search T.right;\n\nRuntime to complete search on a bushy BST in the worst case is $O(log_2(n))$. This is intuitive to see and you can generally estimate the number of computations needed to search a BST by the height of the tree.\\\n\nRemember, the height of the tree is roughly $log_2(n)$. So if the tree is 15 nodes, $log_2(15) = 4$. So the worst case runtime is $O(4) = O(1)$.\n\nBushy BSTs are extremely fast.\n\nAt 1 microsecond per operation, can find something from a tree with $10^300000$ in one second. \n\nMuch (perhaps most?) computation is dedicated towards finding things in response to queries. \n\n# Insert \n\nSearch for key. \n- If found, do nothing.\n- If not found \n  - Create a new node\n  - Set appropiate link. \n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/private/ds/disjointsets":{"title":"Disjoint Sets","content":"\n\n```\nclass DisjointSet {\n    ArrayList\u003cArrayList\u003cInteger\u003e\u003e nums = new ArrayList\u003cArrayList\u003cInteger\u003e\u003e();\n    \n    public DisjointSet(int length){\n        for(int i = 0; i \u003c length; i++){\n            ArrayList\u003cInteger\u003e num = new ArrayList\u003cInteger\u003e();\n            num.add(i);\n            nums.add(num);\n        }\n    }\n    \n    \n    public void printSet(){\n        System.out.print(\"{\");\n        for(int i = 0; i \u003c nums.size(); i++){\n            System.out.print(\"{\");\n            for(int j = 0; j \u003c nums.get(i).size(); j++){\n                if(j+1 \u003e= nums.get(i).size()){\n                    System.out.print(nums.get(i).get(j));\n                } else {\n                    System.out.print(nums.get(i).get(j) + \",\");\n                }\n            }\n            System.out.print(\"}\");\n        }\n        System.out.println(\"}\");\n    }\n    \n    public void connect(int x, int y){\n        for(int i = 0; i \u003c nums.size(); i++){\n            if(nums.get(i).contains(x)){\n                nums.get(i).add(y);\n                i++;\n            } else if (nums.get(i).contains(y)){\n                nums.remove(i);\n            }\n        }    \n    }\n    \n    public boolean isConnected(int x, int y){\n         for(int i = 0; i \u003c nums.size(); i++){\n             if(nums.get(i).contains(x) \u0026\u0026 nums.get(i).contains(y)){\n                 return true;\n             }\n         }\n         return false;\n    }\n    \n\n    \n    public static void main(String[] args){\n        DisjointSet set = new DisjointSet(10);\n        set.printSet();\n        set.connect(1,3);\n        set.printSet();\n        System.out.println(set.isConnected(4,6));\n        set.connect(3,7);\n        set.printSet();\n        System.out.println(set.isConnected(7,1));\n        set.printSet();\n        set.connect(2,7);\n        set.printSet();\n    }\n    \n}\n```","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/thoughts/netflow":{"title":"A Brief Overview of Netflow","content":"\nOne of the challenges of managing a network is understanding what's actually going through that network. We have a lot of different protocols buzzing around - HTTPS, SMTP, SFTP, DNS, etc - so how do we answer questions like how much of each protocol is happening? Who are the top talkers? What are the most visited destinations? \n\n# Lights, Camera, Action!\nThis is where Netflow improves our mortal lives. If you want to monitor your network traffic and see if there are any anomalies - worry no further. If you want to just see what your traffic patterns look like - with netflow it's trivial. \n\nThere are three basic ingredients:\n- A monitor that looks at network traffic as it passes in and out of an unsuspecting router. This traffic is cached in memory for a short period of time.\n- An exporter that takes the recorded traffic data and puts it into a network management system\n- A collector that runs on the network management system to do things like visualize the data.  \n\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null},"/thoughts/rereading":{"title":"Re-Reading","content":"\nFrom time to time I open books that I've already crawled through and squeezed once more. It often feels like a conversation with an old friend: the hurried catch-up, the gradual memories spurring from a long time past, the way their charisma and language just hit you and you're questioning what you're doing in life. When I look for inspiration to write - to storytell - I always go back to Paul Kalanithi's *When Breath Becomes Air*. \n\n# Memorable Sentences\n\n\u003e Even working on the dead, with their faces covered, their names a mystery, you find that their humanity pops up at you —in opening\n\u003e my cadaver's stomach, I found two undigested morphine pills, meaning that he had died in pain, perhaps alone and fumbling with the cap of a pill\n\u003e bottle. — pg. 47\n\n\u003e Once while showing us the ruin's of our donor's pancreatic cancer, the professor asked, \"How old is this fellow?\" \u003cbr\u003e\n\u003e \"Seventy-four,\" we replied. \u003cbr\u003e\n\u003e \"That's my age,\" he said, set down the probe, and walked away — pg. 50\n\n\u003e The secret is to know that the deck is stacked, that you will lose, that your hands or judgement will slip, and yet still struggle to win for \n\u003e your patients. You can't ever reach perfection, but you can believe in an asymptote toward which you are creaselessly striving.\n\n","lastmodified":"2023-04-04T19:05:28.961577979Z","tags":null}}